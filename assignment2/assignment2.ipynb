{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0b933634",
   "metadata": {
    "tags": []
   },
   "source": [
    "# IDS Assignment Part 2 - <font color=\"red\"><h7>Deadline: 23/01/2023 23:59</h7></font>\n",
    "This is the second part of the assignment in IDS 2022/2023. \n",
    "Please use this Jupyter notebook to work on the questions posed in the assignment. When you are done, upload the notebook in Moodle at the designated activity. In addition to the _Jupyter notebook_, please submit _one zip-file_ containing your screenshots for Question 7. \n",
    "\n",
    "Give your commented Python code and answers in the corresponding provided cells. Make sure to answer all questions in a clear and explicit manner and discuss your outputs. _Please do not change the general structure of this notebook_. You can, however, add additional markdown or code cells if necessary. <b>Please DO NOT CLEAR THE OUTPUT of the notebook you are submitting! </b>\n",
    "\n",
    "<font color=\"red\"> *Please make sure to include the names and matriculation numbers of all group members in the slot provided below.* </font> If a name or a student id is missing, the student will not receive any points.\n",
    "\n",
    "Hint 1: While working on the assignment, you will get a better understanding of the dataset. Feel free to generate additional results and visualizations to support your answers. For example, this might be useful regarding data modification, data simplification, or output interpretation. <font color=\"red\">Ensure that all your claims are supported.</font>\n",
    "\n",
    "Hint 2: <font color=\"red\">Plan your time wisely. </font> A few parts of this assignment may take some time to run. It might be necessary to consider time management when you plan your group work. Also, do not attempt to upload your assignment at the last minute before the deadline. This often does not work, and you will miss the deadline. Late submissions will not be considered.\n",
    "\n",
    "Hint 3: RWTHmoodle allows multiple submissions, with every new submission overwriting the previous one. <b>Partial submissions are therefore possible and encouraged. </b> This might be helpful in case of technical issues with RWTHMoodle, which may occur close to the deadline."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76db0f12",
   "metadata": {},
   "source": [
    "<font color=\"red\"><b>Student Names and IDs:\n",
    "    \n",
    "    1. Minh Nghia Phan 394806\n",
    "    \n",
    "    2. \n",
    "    \n",
    "    3."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7803d7da-69e1-412a-8a9f-c04ceb96a0f2",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Question 1: Preprocessing (20 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "a3c5e97f-99a5-41d4-92a8-e9888bf4f4c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "### Sklearn\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "from sklearn.impute import KNNImputer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ffab9bb-d08f-44dd-ad3f-ac3dc8e2184c",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "In this question, we consider a dataset documenting the Ski Resorts in Europe (**ski_resorts.csv**).\n",
    "Each row contains some information about the Ski resort.\n",
    "You can find a short description for each column:\n",
    "\n",
    "| Column | Description |\n",
    "| ------ | ----------- |\n",
    "| Resort | The name of the ski & snowboard resort. |\n",
    "| Country | The country in which the resort is located. |\n",
    "| HighestPoint | The highest mountain point at the ski resort.   |\n",
    "| LowestPoint | The lowest possible point to ski at the ski resort.  |\n",
    "| DayPassPriceAdult | The price shows what it costs for 1 adult for 1 day in the main season in Euro. |\n",
    "| BeginnerSlope | The total amount of “beginner” slopes in kilometer at the resort. “Beginner slopes” contains “children”, “blue” and, “green” slopes. |\n",
    "| IntermediateSlope | The total amount of “intermediate” slopes in kilometer at the resort. “Intermediate slopes” contains “red” slopes. |\n",
    "| DifficultSlope | The total amount of “difficult” slopes in kilometer at the resort. “Difficult slopes” contains “black”, “advanced”, and ”expert” slopes. |\n",
    "| TotalSlope | The sum of “beginner slopes” + “intermediate slopes” + “difficult slopes” |\n",
    "| Snowparks | Does the resort have one or more snowparks, or not? |\n",
    "| NightSki | Does the resort offer skiing on illuminated slopes? |\n",
    "| SurfaceLifts | The amount of lifts in this category: T-bar, Sunkidslift, Rope lifts, and people mower. |\n",
    "| ChairLifts | The total amount of chairlifts. |\n",
    "| GondolaLifts | The amount of lifts in this category: Gondola, Train lifts, Funicular, Combined gondola and chairlifts, Helicopter lifts, Snowcats, and Aerial tramways. |\n",
    "| TotalLifts | The sum of “surface lifts etc” + “gondola etc” + “chairlifts etc.” |\n",
    "| LiftCapacity | How many passengers can the lift system at the ski resort mowe in one hour? |\n",
    "| SnowCannons  |The total amount of snow cannons at the ski resort.|"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e7bd4f6-32a5-4e9d-9339-00556a268506",
   "metadata": {
    "tags": []
   },
   "source": [
    "### a) Loading the Data and Initial Quality Investigation (2.5pts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82c39c84-0399-4369-ad4d-c657c08aefba",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### **a(i)** \n",
    "Load the dataset into a dataframe `df`. <font color='red'>Use the first column as index for your dataframe</font>. Ensure that the index is valid, that is, it should not contain any duplicate entries. \n",
    "\n",
    "\n",
    "\n",
    "**In the subsequent questions, only modify the dataframe `df` if explicitly requested. However, you can always create working copies.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "897a95f5-c50d-4240-b357-65ff937034ba",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Resort</th>\n",
       "      <th>Country</th>\n",
       "      <th>HighestPoint</th>\n",
       "      <th>LowestPoint</th>\n",
       "      <th>DayPassPriceAdult</th>\n",
       "      <th>BeginnerSlope</th>\n",
       "      <th>IntermediateSlope</th>\n",
       "      <th>DifficultSlope</th>\n",
       "      <th>TotalSlope</th>\n",
       "      <th>Snowparks</th>\n",
       "      <th>NightSki</th>\n",
       "      <th>SurfaceLifts</th>\n",
       "      <th>ChairLifts</th>\n",
       "      <th>GondolaLifts</th>\n",
       "      <th>TotalLifts</th>\n",
       "      <th>LiftCapacity</th>\n",
       "      <th>SnowCannons</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Alpendorf (Ski amedé)</td>\n",
       "      <td>Austria</td>\n",
       "      <td>1980.0</td>\n",
       "      <td>740.0</td>\n",
       "      <td>52.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>81</td>\n",
       "      <td>4.0</td>\n",
       "      <td>115</td>\n",
       "      <td>Yes</td>\n",
       "      <td>No</td>\n",
       "      <td>22.0</td>\n",
       "      <td>16</td>\n",
       "      <td>11.0</td>\n",
       "      <td>49</td>\n",
       "      <td>75398.0</td>\n",
       "      <td>600.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Soldeu-Pas de la Casa/​Grau Roig/​El Tarter/​C...</td>\n",
       "      <td>Andorra</td>\n",
       "      <td>2640.0</td>\n",
       "      <td>1710.0</td>\n",
       "      <td>47.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>77</td>\n",
       "      <td>33.0</td>\n",
       "      <td>210</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Yes</td>\n",
       "      <td>37.0</td>\n",
       "      <td>28</td>\n",
       "      <td>7.0</td>\n",
       "      <td>72</td>\n",
       "      <td>99017.0</td>\n",
       "      <td>1032.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Oberau (Wildschönau)</td>\n",
       "      <td>Austria</td>\n",
       "      <td>1130.0</td>\n",
       "      <td>900.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2</td>\n",
       "      <td>1932.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Dachstein West</td>\n",
       "      <td>Austria</td>\n",
       "      <td>1620.0</td>\n",
       "      <td>780.0</td>\n",
       "      <td>42.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>33</td>\n",
       "      <td>3.0</td>\n",
       "      <td>51</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Yes</td>\n",
       "      <td>25.0</td>\n",
       "      <td>8</td>\n",
       "      <td>3.0</td>\n",
       "      <td>36</td>\n",
       "      <td>32938.0</td>\n",
       "      <td>163.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Rosa Khutor</td>\n",
       "      <td>Southern Russia</td>\n",
       "      <td>2320.0</td>\n",
       "      <td>940.0</td>\n",
       "      <td>22.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>26</td>\n",
       "      <td>21.0</td>\n",
       "      <td>77</td>\n",
       "      <td>Yes</td>\n",
       "      <td>No</td>\n",
       "      <td>6.0</td>\n",
       "      <td>11</td>\n",
       "      <td>10.0</td>\n",
       "      <td>27</td>\n",
       "      <td>49228.0</td>\n",
       "      <td>450.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>372</th>\n",
       "      <td>Montgenèvre (Via Lattea)</td>\n",
       "      <td>France</td>\n",
       "      <td>2749.0</td>\n",
       "      <td>1372.0</td>\n",
       "      <td>48.0</td>\n",
       "      <td>96.0</td>\n",
       "      <td>220</td>\n",
       "      <td>84.0</td>\n",
       "      <td>400</td>\n",
       "      <td>No</td>\n",
       "      <td>Yes</td>\n",
       "      <td>NaN</td>\n",
       "      <td>35</td>\n",
       "      <td>7.0</td>\n",
       "      <td>71</td>\n",
       "      <td>96433.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>373</th>\n",
       "      <td>Sauze d’Oulx (Via Lattea)</td>\n",
       "      <td>Italy</td>\n",
       "      <td>2749.0</td>\n",
       "      <td>1372.0</td>\n",
       "      <td>48.0</td>\n",
       "      <td>96.0</td>\n",
       "      <td>220</td>\n",
       "      <td>84.0</td>\n",
       "      <td>400</td>\n",
       "      <td>No</td>\n",
       "      <td>Yes</td>\n",
       "      <td>29.0</td>\n",
       "      <td>35</td>\n",
       "      <td>7.0</td>\n",
       "      <td>71</td>\n",
       "      <td>96433.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>374</th>\n",
       "      <td>Gressoney - La-Trinite (Monterosa Ski)</td>\n",
       "      <td>Italy</td>\n",
       "      <td>3275.0</td>\n",
       "      <td>1212.0</td>\n",
       "      <td>43.0</td>\n",
       "      <td>23.0</td>\n",
       "      <td>94</td>\n",
       "      <td>15.0</td>\n",
       "      <td>132</td>\n",
       "      <td>Yes</td>\n",
       "      <td>No</td>\n",
       "      <td>9.0</td>\n",
       "      <td>9</td>\n",
       "      <td>12.0</td>\n",
       "      <td>30</td>\n",
       "      <td>31984.0</td>\n",
       "      <td>655.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>375</th>\n",
       "      <td>Champoluc (Monterosa Ski)</td>\n",
       "      <td>Italy</td>\n",
       "      <td>3275.0</td>\n",
       "      <td>1212.0</td>\n",
       "      <td>43.0</td>\n",
       "      <td>23.0</td>\n",
       "      <td>94</td>\n",
       "      <td>15.0</td>\n",
       "      <td>132</td>\n",
       "      <td>Yes</td>\n",
       "      <td>No</td>\n",
       "      <td>9.0</td>\n",
       "      <td>9</td>\n",
       "      <td>12.0</td>\n",
       "      <td>30</td>\n",
       "      <td>31984.0</td>\n",
       "      <td>655.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>376</th>\n",
       "      <td>Zauchensee</td>\n",
       "      <td>Austria</td>\n",
       "      <td>2188.0</td>\n",
       "      <td>1000.0</td>\n",
       "      <td>52.0</td>\n",
       "      <td>23.0</td>\n",
       "      <td>16</td>\n",
       "      <td>4.0</td>\n",
       "      <td>44</td>\n",
       "      <td>Yes</td>\n",
       "      <td>No</td>\n",
       "      <td>9.0</td>\n",
       "      <td>6</td>\n",
       "      <td>4.0</td>\n",
       "      <td>19</td>\n",
       "      <td>25988.0</td>\n",
       "      <td>113.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>376 rows × 17 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                Resort          Country  \\\n",
       "1                                Alpendorf (Ski amedé)          Austria   \n",
       "2    Soldeu-Pas de la Casa/​Grau Roig/​El Tarter/​C...          Andorra   \n",
       "3                                Oberau (Wildschönau)          Austria   \n",
       "4                                       Dachstein West          Austria   \n",
       "5                                          Rosa Khutor  Southern Russia   \n",
       "..                                                 ...              ...   \n",
       "372                          Montgenèvre (Via Lattea)           France   \n",
       "373                          Sauze d’Oulx (Via Lattea)            Italy   \n",
       "374             Gressoney - La-Trinite (Monterosa Ski)            Italy   \n",
       "375                          Champoluc (Monterosa Ski)            Italy   \n",
       "376                                         Zauchensee          Austria   \n",
       "\n",
       "     HighestPoint  LowestPoint  DayPassPriceAdult  BeginnerSlope  \\\n",
       "1          1980.0        740.0               52.0           30.0   \n",
       "2          2640.0       1710.0               47.0          100.0   \n",
       "3          1130.0        900.0               30.0            1.0   \n",
       "4          1620.0        780.0               42.0           15.0   \n",
       "5          2320.0        940.0               22.0           30.0   \n",
       "..            ...          ...                ...            ...   \n",
       "372        2749.0       1372.0               48.0           96.0   \n",
       "373        2749.0       1372.0               48.0           96.0   \n",
       "374        3275.0       1212.0               43.0           23.0   \n",
       "375        3275.0       1212.0               43.0           23.0   \n",
       "376        2188.0       1000.0               52.0           23.0   \n",
       "\n",
       "     IntermediateSlope  DifficultSlope  TotalSlope Snowparks NightSki  \\\n",
       "1                   81             4.0         115       Yes       No   \n",
       "2                   77            33.0         210       Yes      Yes   \n",
       "3                    0             1.0           2        No       No   \n",
       "4                   33             3.0          51       Yes      Yes   \n",
       "5                   26            21.0          77       Yes       No   \n",
       "..                 ...             ...         ...       ...      ...   \n",
       "372                220            84.0         400        No      Yes   \n",
       "373                220            84.0         400        No      Yes   \n",
       "374                 94            15.0         132       Yes       No   \n",
       "375                 94            15.0         132       Yes       No   \n",
       "376                 16             4.0          44       Yes       No   \n",
       "\n",
       "     SurfaceLifts  ChairLifts  GondolaLifts  TotalLifts  LiftCapacity  \\\n",
       "1            22.0          16          11.0          49       75398.0   \n",
       "2            37.0          28           7.0          72       99017.0   \n",
       "3             2.0           0           0.0           2        1932.0   \n",
       "4            25.0           8           3.0          36       32938.0   \n",
       "5             6.0          11          10.0          27       49228.0   \n",
       "..            ...         ...           ...         ...           ...   \n",
       "372           NaN          35           7.0          71       96433.0   \n",
       "373          29.0          35           7.0          71       96433.0   \n",
       "374           9.0           9          12.0          30       31984.0   \n",
       "375           9.0           9          12.0          30       31984.0   \n",
       "376           9.0           6           4.0          19       25988.0   \n",
       "\n",
       "     SnowCannons  \n",
       "1          600.0  \n",
       "2         1032.0  \n",
       "3            0.0  \n",
       "4          163.0  \n",
       "5          450.0  \n",
       "..           ...  \n",
       "372          0.0  \n",
       "373          0.0  \n",
       "374        655.0  \n",
       "375        655.0  \n",
       "376        113.0  \n",
       "\n",
       "[376 rows x 17 columns]"
      ]
     },
     "execution_count": 166,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('datasets/ski_resorts.csv', index_col=0)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca4b69ca-8603-4cf9-8e6a-32bea8b70a32",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-07T10:01:29.281703Z",
     "iopub.status.busy": "2022-12-07T10:01:29.281301Z",
     "iopub.status.idle": "2022-12-07T10:01:29.305867Z",
     "shell.execute_reply": "2022-12-07T10:01:29.304586Z",
     "shell.execute_reply.started": "2022-12-07T10:01:29.281673Z"
    },
    "tags": []
   },
   "source": [
    "#### **a(ii)** \n",
    "Show the data types of the dataframe columns as well as the first 5 rows. On the first sight, are there any data type problems (e.g., numerical columns having a non-numerical data type)?\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "8a2e18e6-f590-449d-891f-f5684fdb23e2",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Resort                object\n",
       "Country               object\n",
       "HighestPoint         float64\n",
       "LowestPoint          float64\n",
       "DayPassPriceAdult    float64\n",
       "BeginnerSlope        float64\n",
       "IntermediateSlope      int64\n",
       "DifficultSlope       float64\n",
       "TotalSlope             int64\n",
       "Snowparks             object\n",
       "NightSki              object\n",
       "SurfaceLifts         float64\n",
       "ChairLifts             int64\n",
       "GondolaLifts         float64\n",
       "TotalLifts             int64\n",
       "LiftCapacity         float64\n",
       "SnowCannons          float64\n",
       "dtype: object"
      ]
     },
     "execution_count": 167,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Show column data types\n",
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "15dc2c79",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Resort</th>\n",
       "      <th>Country</th>\n",
       "      <th>HighestPoint</th>\n",
       "      <th>LowestPoint</th>\n",
       "      <th>DayPassPriceAdult</th>\n",
       "      <th>BeginnerSlope</th>\n",
       "      <th>IntermediateSlope</th>\n",
       "      <th>DifficultSlope</th>\n",
       "      <th>TotalSlope</th>\n",
       "      <th>Snowparks</th>\n",
       "      <th>NightSki</th>\n",
       "      <th>SurfaceLifts</th>\n",
       "      <th>ChairLifts</th>\n",
       "      <th>GondolaLifts</th>\n",
       "      <th>TotalLifts</th>\n",
       "      <th>LiftCapacity</th>\n",
       "      <th>SnowCannons</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Alpendorf (Ski amedé)</td>\n",
       "      <td>Austria</td>\n",
       "      <td>1980.0</td>\n",
       "      <td>740.0</td>\n",
       "      <td>52.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>81</td>\n",
       "      <td>4.0</td>\n",
       "      <td>115</td>\n",
       "      <td>Yes</td>\n",
       "      <td>No</td>\n",
       "      <td>22.0</td>\n",
       "      <td>16</td>\n",
       "      <td>11.0</td>\n",
       "      <td>49</td>\n",
       "      <td>75398.0</td>\n",
       "      <td>600.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Soldeu-Pas de la Casa/​Grau Roig/​El Tarter/​C...</td>\n",
       "      <td>Andorra</td>\n",
       "      <td>2640.0</td>\n",
       "      <td>1710.0</td>\n",
       "      <td>47.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>77</td>\n",
       "      <td>33.0</td>\n",
       "      <td>210</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Yes</td>\n",
       "      <td>37.0</td>\n",
       "      <td>28</td>\n",
       "      <td>7.0</td>\n",
       "      <td>72</td>\n",
       "      <td>99017.0</td>\n",
       "      <td>1032.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Oberau (Wildschönau)</td>\n",
       "      <td>Austria</td>\n",
       "      <td>1130.0</td>\n",
       "      <td>900.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2</td>\n",
       "      <td>1932.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Dachstein West</td>\n",
       "      <td>Austria</td>\n",
       "      <td>1620.0</td>\n",
       "      <td>780.0</td>\n",
       "      <td>42.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>33</td>\n",
       "      <td>3.0</td>\n",
       "      <td>51</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Yes</td>\n",
       "      <td>25.0</td>\n",
       "      <td>8</td>\n",
       "      <td>3.0</td>\n",
       "      <td>36</td>\n",
       "      <td>32938.0</td>\n",
       "      <td>163.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Rosa Khutor</td>\n",
       "      <td>Southern Russia</td>\n",
       "      <td>2320.0</td>\n",
       "      <td>940.0</td>\n",
       "      <td>22.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>26</td>\n",
       "      <td>21.0</td>\n",
       "      <td>77</td>\n",
       "      <td>Yes</td>\n",
       "      <td>No</td>\n",
       "      <td>6.0</td>\n",
       "      <td>11</td>\n",
       "      <td>10.0</td>\n",
       "      <td>27</td>\n",
       "      <td>49228.0</td>\n",
       "      <td>450.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              Resort          Country  \\\n",
       "1                              Alpendorf (Ski amedé)          Austria   \n",
       "2  Soldeu-Pas de la Casa/​Grau Roig/​El Tarter/​C...          Andorra   \n",
       "3                              Oberau (Wildschönau)          Austria   \n",
       "4                                     Dachstein West          Austria   \n",
       "5                                        Rosa Khutor  Southern Russia   \n",
       "\n",
       "   HighestPoint  LowestPoint  DayPassPriceAdult  BeginnerSlope  \\\n",
       "1        1980.0        740.0               52.0           30.0   \n",
       "2        2640.0       1710.0               47.0          100.0   \n",
       "3        1130.0        900.0               30.0            1.0   \n",
       "4        1620.0        780.0               42.0           15.0   \n",
       "5        2320.0        940.0               22.0           30.0   \n",
       "\n",
       "   IntermediateSlope  DifficultSlope  TotalSlope Snowparks NightSki  \\\n",
       "1                 81             4.0         115       Yes       No   \n",
       "2                 77            33.0         210       Yes      Yes   \n",
       "3                  0             1.0           2        No       No   \n",
       "4                 33             3.0          51       Yes      Yes   \n",
       "5                 26            21.0          77       Yes       No   \n",
       "\n",
       "   SurfaceLifts  ChairLifts  GondolaLifts  TotalLifts  LiftCapacity  \\\n",
       "1          22.0          16          11.0          49       75398.0   \n",
       "2          37.0          28           7.0          72       99017.0   \n",
       "3           2.0           0           0.0           2        1932.0   \n",
       "4          25.0           8           3.0          36       32938.0   \n",
       "5           6.0          11          10.0          27       49228.0   \n",
       "\n",
       "   SnowCannons  \n",
       "1        600.0  \n",
       "2       1032.0  \n",
       "3          0.0  \n",
       "4        163.0  \n",
       "5        450.0  "
      ]
     },
     "execution_count": 168,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# First 5 rows\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34febc32-6c1a-45ec-96c5-f81dda43ba7b",
   "metadata": {},
   "source": [
    "**Your Answer:** \n",
    "1. The columns **IntermediateSlope** and **TotalSlope** measure the amount of slopes in kilometers and therefore should have types __float64__ instead of __int64__.\n",
    "\n",
    "2. **Snowparks** and **NightSki** should have __boolean__ type because they only have *Yes* and *No* values.\n",
    "\n",
    "3. **SurfaceLifts**, **GondolaLifts**, **LiftCapacity**, and **SnowCannons** should have type __int64__ instead of __float64__ because they indicate either number of lifts or number of passengers, which should all be integers."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8ff406b-5ec1-4b26-8abb-5b5a91643841",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### **a(iii)** \n",
    "To improve performance and memory usage (in particular for large datasets) it is important to use **categorical** columns whenever suitable.\n",
    "Are there any categorical column candidates? Explain your answer. \\\n",
    "Afterward, convert the column(s) in `df` into categorical column(s)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "afa75081-bf63-4e85-94c2-483d9ae4118e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Convert to categorical\n",
    "tocat = ['Country', 'Snowparks', 'NightSki']\n",
    "for col in tocat:\n",
    "    df[col] = df[col].astype('category')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "2d1c8296",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "376"
      ]
     },
     "execution_count": 170,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Number of unique resorts\n",
    "len(df['Resort'].unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "131c3379-bb44-42d0-9c4f-9d1e4c2d38b6",
   "metadata": {},
   "source": [
    "**Your Answer:** **Country** can be categorical, as each country is a category itself.\n",
    "\n",
    "The other columns, other than **Resort**, **Snowparks** and **NightSki**, can not be categorical, because they represent numerical real values and measure magnitudes (kilometers, prices, number of lifts/passengers).\n",
    "\n",
    "**Snowparks** and **NightSki** should be boolean instead of categorical, because they have only 2 values \"Yes\" and \"No\".\n",
    "\n",
    "**Resort** represents the unique name of each resort in each row (there are 376 unique resorts corresponding to the exact 376 rows of the dataframe), hence it should not be categorical."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0222a54c-f455-4b78-bae1-33bc7d79edf3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-07T16:58:43.496952Z",
     "iopub.status.busy": "2022-12-07T16:58:43.496547Z",
     "iopub.status.idle": "2022-12-07T16:58:43.506339Z",
     "shell.execute_reply": "2022-12-07T16:58:43.504499Z",
     "shell.execute_reply.started": "2022-12-07T16:58:43.496921Z"
    },
    "tags": []
   },
   "source": [
    "### b) Handling Missing Values & Encoding (17.5pts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "279b17dc-acae-4012-b83b-2bae628a8fb4",
   "metadata": {},
   "source": [
    "In the following task, you can assume that every NAN entry in the dataframe is actually a missing value. This can partially be justified by the fact that pandas did not have problems inferring the \"proper\" datatypes (e.g., a string indicating a missing number in a number column would result in pandas parsing an object column) and your subsequent check of the data types. Therefore, you can use `df.isna()` as a proxy indicator for missing values."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a760130-8c4b-46b6-b96f-76dcea95d28e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-07T16:59:35.630762Z",
     "iopub.status.busy": "2022-12-07T16:59:35.630357Z",
     "iopub.status.idle": "2022-12-07T16:59:35.648425Z",
     "shell.execute_reply": "2022-12-07T16:59:35.646703Z",
     "shell.execute_reply.started": "2022-12-07T16:59:35.630730Z"
    },
    "tags": []
   },
   "source": [
    "#### **b(i)** \n",
    "Simply discarding missing entries is usually not a good idea. Therefore, you should first analyze the number of missing values and check for patterns of missing values. \n",
    "\n",
    "To this end, compute the following statistics on missing values:\n",
    "1. How many entries does the dataframe have? (To relate this to the number of entries missing)\n",
    "2. How many missing values do we have? What is the ratio i.e., \"number of missing values\"/\"number of entries of df\"?\n",
    "3. How many rows have at least a single missing value?\n",
    "4. Count the number of missing values per column.\n",
    "5. Count the number of missing values per row and aggregate them - i.e., show the number of rows that suffer from x missing values.\n",
    "6. What do you observe? Are there any rows containing missing values for the same set of columns? Can you identify potential patterns?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "id": "c0b7c69e-8c4e-483e-9131-9b7f2936216c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of entries in the dataframe: 6392\n"
     ]
    }
   ],
   "source": [
    "# your code for 1. How many entries does the dataframe have? (To relate this to the number of entries missing)\n",
    "n_entries = len(df.columns)*len(df.index)\n",
    "print(f\"Number of entries in the dataframe: {n_entries}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "id": "e10f80a7-4caf-4d0b-8cf8-c3148e466288",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of missing values in the dataframe: 87\n",
      "Ratio of missing values: 0.013610763454317897\n"
     ]
    }
   ],
   "source": [
    "# your code for 2. How many missing values do we have? What is the ratio i.e., \"number of missing values\"/\"number of entries of df\"?\n",
    "n_missing = df.isnull().sum().sum()\n",
    "print(f\"Number of missing values in the dataframe: {n_missing}\")\n",
    "print(f\"Ratio of missing values: {n_missing/n_entries}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "id": "d4ecc58b-6563-4dd1-8727-4aa08a485961",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows with at least 1 missing value: 45\n"
     ]
    }
   ],
   "source": [
    "# your code for 3. How many rows have at least a single missing value?\n",
    "df_rows_na = df[df.isna().any(axis=1)] # DF of rows with at least 1 NaN value\n",
    "print(f\"Number of rows with at least 1 missing value: {len(df_rows_na)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "id": "55e8a812-67c1-4458-ac9e-41265cab70c3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of missing values per columns\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Resort                0\n",
       "Country               0\n",
       "HighestPoint         13\n",
       "LowestPoint           3\n",
       "DayPassPriceAdult    17\n",
       "BeginnerSlope         8\n",
       "IntermediateSlope     0\n",
       "DifficultSlope        8\n",
       "TotalSlope            0\n",
       "Snowparks             0\n",
       "NightSki              0\n",
       "SurfaceLifts          6\n",
       "ChairLifts            0\n",
       "GondolaLifts          7\n",
       "TotalLifts            0\n",
       "LiftCapacity         17\n",
       "SnowCannons           8\n",
       "dtype: int64"
      ]
     },
     "execution_count": 174,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# your code for 4. Count the number of missing values per column.\n",
    "print(f\"Number of missing values per columns\")\n",
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "id": "724f6928-edfe-4fec-b3fd-abfc6567d1ec",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Left column: number of missing values, right: number of rows with that many missing values\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0    331\n",
       "1     35\n",
       "6      8\n",
       "2      2\n",
       "dtype: int64"
      ]
     },
     "execution_count": 175,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# your code for 5. Count the number of missing values per row and aggregate them - i.e., show the number of rows that suffer from x missing values.\n",
    "df_missing = df.isna().astype(\"int\") # entry=1 indicates missing values, else 0\n",
    "df_missing = df_missing.apply(np.sum, axis=1) # sum all rows\n",
    "df_missing = df_missing.value_counts() # aggregate by num of missing values\n",
    "print('Left column: number of missing values, right: number of rows with that many missing values')\n",
    "df_missing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fc8a782-1430-4e7c-835c-91b7e1806597",
   "metadata": {
    "tags": []
   },
   "source": [
    "**Your answer:** *(for 6. What do you observe? Are there any rows containing missing values for the same set of columns?)*\n",
    "\n",
    "We observe that there are 8 rows with 6 missing values. From 4., we can see that there are exactly 6 columns with at least 8 missing values (HighestPoint, DayPassPriceAdult, BeginnerSlope, DifficultSlope, LiftCapacity, and SnowCannons). So the missing values of these rows are all from these 6 columns."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5467e51c-6ba1-47f6-a123-fd55076b8e45",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### **b(ii)**\n",
    "For the next step:\n",
    "\n",
    "1. Transform the categorical column(s) you identified in a(iii) into one-hot encoding format. \n",
    "2. Transform the columns \"Snowparks\" and \"NightSki\" in `df` into boolean data type, where \"Yes\" should be `True` and \"No\" should be `False`\n",
    "\n",
    "In the end, the original categorical column(s) should still be there. Additionally, there should be x number (x is the number of unique values) of one-hot encoding columns for each categorical column. Use the following naming convention for the new columns \"{name of the categorical column}_{unique value for that column}\" Also, make sure the columns \"Snowparks\" and \"NightSki\" are boolean type in the end.\n",
    "Lastly, print the top five rows of the resulting dataframe.\n",
    "\n",
    "*Hint: You can use the pd.get_dummies() function from pandas for the first transformation*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "id": "d4098a7f-2640-4577-9fe3-09e0bf7519b2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Resort</th>\n",
       "      <th>HighestPoint</th>\n",
       "      <th>LowestPoint</th>\n",
       "      <th>DayPassPriceAdult</th>\n",
       "      <th>BeginnerSlope</th>\n",
       "      <th>IntermediateSlope</th>\n",
       "      <th>DifficultSlope</th>\n",
       "      <th>TotalSlope</th>\n",
       "      <th>Snowparks</th>\n",
       "      <th>NightSki</th>\n",
       "      <th>...</th>\n",
       "      <th>Country_Siberia</th>\n",
       "      <th>Country_Slovakia</th>\n",
       "      <th>Country_Slovenia</th>\n",
       "      <th>Country_Southern Russia</th>\n",
       "      <th>Country_Spain</th>\n",
       "      <th>Country_Sweden</th>\n",
       "      <th>Country_Switzerland</th>\n",
       "      <th>Country_Ukraine</th>\n",
       "      <th>Country_United Kingdom</th>\n",
       "      <th>Country</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Alpendorf (Ski amedé)</td>\n",
       "      <td>1980.0</td>\n",
       "      <td>740.0</td>\n",
       "      <td>52.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>81</td>\n",
       "      <td>4.0</td>\n",
       "      <td>115</td>\n",
       "      <td>Yes</td>\n",
       "      <td>No</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Austria</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Soldeu-Pas de la Casa/​Grau Roig/​El Tarter/​C...</td>\n",
       "      <td>2640.0</td>\n",
       "      <td>1710.0</td>\n",
       "      <td>47.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>77</td>\n",
       "      <td>33.0</td>\n",
       "      <td>210</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Yes</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Andorra</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Oberau (Wildschönau)</td>\n",
       "      <td>1130.0</td>\n",
       "      <td>900.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Austria</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Dachstein West</td>\n",
       "      <td>1620.0</td>\n",
       "      <td>780.0</td>\n",
       "      <td>42.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>33</td>\n",
       "      <td>3.0</td>\n",
       "      <td>51</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Yes</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Austria</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Rosa Khutor</td>\n",
       "      <td>2320.0</td>\n",
       "      <td>940.0</td>\n",
       "      <td>22.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>26</td>\n",
       "      <td>21.0</td>\n",
       "      <td>77</td>\n",
       "      <td>Yes</td>\n",
       "      <td>No</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Southern Russia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>372</th>\n",
       "      <td>Montgenèvre (Via Lattea)</td>\n",
       "      <td>2749.0</td>\n",
       "      <td>1372.0</td>\n",
       "      <td>48.0</td>\n",
       "      <td>96.0</td>\n",
       "      <td>220</td>\n",
       "      <td>84.0</td>\n",
       "      <td>400</td>\n",
       "      <td>No</td>\n",
       "      <td>Yes</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>France</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>373</th>\n",
       "      <td>Sauze d’Oulx (Via Lattea)</td>\n",
       "      <td>2749.0</td>\n",
       "      <td>1372.0</td>\n",
       "      <td>48.0</td>\n",
       "      <td>96.0</td>\n",
       "      <td>220</td>\n",
       "      <td>84.0</td>\n",
       "      <td>400</td>\n",
       "      <td>No</td>\n",
       "      <td>Yes</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Italy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>374</th>\n",
       "      <td>Gressoney - La-Trinite (Monterosa Ski)</td>\n",
       "      <td>3275.0</td>\n",
       "      <td>1212.0</td>\n",
       "      <td>43.0</td>\n",
       "      <td>23.0</td>\n",
       "      <td>94</td>\n",
       "      <td>15.0</td>\n",
       "      <td>132</td>\n",
       "      <td>Yes</td>\n",
       "      <td>No</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Italy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>375</th>\n",
       "      <td>Champoluc (Monterosa Ski)</td>\n",
       "      <td>3275.0</td>\n",
       "      <td>1212.0</td>\n",
       "      <td>43.0</td>\n",
       "      <td>23.0</td>\n",
       "      <td>94</td>\n",
       "      <td>15.0</td>\n",
       "      <td>132</td>\n",
       "      <td>Yes</td>\n",
       "      <td>No</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Italy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>376</th>\n",
       "      <td>Zauchensee</td>\n",
       "      <td>2188.0</td>\n",
       "      <td>1000.0</td>\n",
       "      <td>52.0</td>\n",
       "      <td>23.0</td>\n",
       "      <td>16</td>\n",
       "      <td>4.0</td>\n",
       "      <td>44</td>\n",
       "      <td>Yes</td>\n",
       "      <td>No</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Austria</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>376 rows × 44 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                Resort  HighestPoint  \\\n",
       "1                                Alpendorf (Ski amedé)        1980.0   \n",
       "2    Soldeu-Pas de la Casa/​Grau Roig/​El Tarter/​C...        2640.0   \n",
       "3                                Oberau (Wildschönau)        1130.0   \n",
       "4                                       Dachstein West        1620.0   \n",
       "5                                          Rosa Khutor        2320.0   \n",
       "..                                                 ...           ...   \n",
       "372                          Montgenèvre (Via Lattea)        2749.0   \n",
       "373                          Sauze d’Oulx (Via Lattea)        2749.0   \n",
       "374             Gressoney - La-Trinite (Monterosa Ski)        3275.0   \n",
       "375                          Champoluc (Monterosa Ski)        3275.0   \n",
       "376                                         Zauchensee        2188.0   \n",
       "\n",
       "     LowestPoint  DayPassPriceAdult  BeginnerSlope  IntermediateSlope  \\\n",
       "1          740.0               52.0           30.0                 81   \n",
       "2         1710.0               47.0          100.0                 77   \n",
       "3          900.0               30.0            1.0                  0   \n",
       "4          780.0               42.0           15.0                 33   \n",
       "5          940.0               22.0           30.0                 26   \n",
       "..           ...                ...            ...                ...   \n",
       "372       1372.0               48.0           96.0                220   \n",
       "373       1372.0               48.0           96.0                220   \n",
       "374       1212.0               43.0           23.0                 94   \n",
       "375       1212.0               43.0           23.0                 94   \n",
       "376       1000.0               52.0           23.0                 16   \n",
       "\n",
       "     DifficultSlope  TotalSlope Snowparks NightSki  ...  Country_Siberia  \\\n",
       "1               4.0         115       Yes       No  ...                0   \n",
       "2              33.0         210       Yes      Yes  ...                0   \n",
       "3               1.0           2        No       No  ...                0   \n",
       "4               3.0          51       Yes      Yes  ...                0   \n",
       "5              21.0          77       Yes       No  ...                0   \n",
       "..              ...         ...       ...      ...  ...              ...   \n",
       "372            84.0         400        No      Yes  ...                0   \n",
       "373            84.0         400        No      Yes  ...                0   \n",
       "374            15.0         132       Yes       No  ...                0   \n",
       "375            15.0         132       Yes       No  ...                0   \n",
       "376             4.0          44       Yes       No  ...                0   \n",
       "\n",
       "     Country_Slovakia  Country_Slovenia  Country_Southern Russia  \\\n",
       "1                   0                 0                        0   \n",
       "2                   0                 0                        0   \n",
       "3                   0                 0                        0   \n",
       "4                   0                 0                        0   \n",
       "5                   0                 0                        1   \n",
       "..                ...               ...                      ...   \n",
       "372                 0                 0                        0   \n",
       "373                 0                 0                        0   \n",
       "374                 0                 0                        0   \n",
       "375                 0                 0                        0   \n",
       "376                 0                 0                        0   \n",
       "\n",
       "     Country_Spain  Country_Sweden  Country_Switzerland  Country_Ukraine  \\\n",
       "1                0               0                    0                0   \n",
       "2                0               0                    0                0   \n",
       "3                0               0                    0                0   \n",
       "4                0               0                    0                0   \n",
       "5                0               0                    0                0   \n",
       "..             ...             ...                  ...              ...   \n",
       "372              0               0                    0                0   \n",
       "373              0               0                    0                0   \n",
       "374              0               0                    0                0   \n",
       "375              0               0                    0                0   \n",
       "376              0               0                    0                0   \n",
       "\n",
       "     Country_United Kingdom          Country  \n",
       "1                         0          Austria  \n",
       "2                         0          Andorra  \n",
       "3                         0          Austria  \n",
       "4                         0          Austria  \n",
       "5                         0  Southern Russia  \n",
       "..                      ...              ...  \n",
       "372                       0           France  \n",
       "373                       0            Italy  \n",
       "374                       0            Italy  \n",
       "375                       0            Italy  \n",
       "376                       0          Austria  \n",
       "\n",
       "[376 rows x 44 columns]"
      ]
     },
     "execution_count": 176,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# your code for 1. Transform the categorical column(s) you identified in a(iii) into one-hot encoding format. \n",
    "df2 = pd.get_dummies(df, columns=['Country'])\n",
    "df2['Country'] = df['Country']\n",
    "df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "id": "381d6bff-270d-4e5a-93e7-62dc97b0551e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Resort</th>\n",
       "      <th>HighestPoint</th>\n",
       "      <th>LowestPoint</th>\n",
       "      <th>DayPassPriceAdult</th>\n",
       "      <th>BeginnerSlope</th>\n",
       "      <th>IntermediateSlope</th>\n",
       "      <th>DifficultSlope</th>\n",
       "      <th>TotalSlope</th>\n",
       "      <th>Snowparks</th>\n",
       "      <th>NightSki</th>\n",
       "      <th>...</th>\n",
       "      <th>Country_Siberia</th>\n",
       "      <th>Country_Slovakia</th>\n",
       "      <th>Country_Slovenia</th>\n",
       "      <th>Country_Southern Russia</th>\n",
       "      <th>Country_Spain</th>\n",
       "      <th>Country_Sweden</th>\n",
       "      <th>Country_Switzerland</th>\n",
       "      <th>Country_Ukraine</th>\n",
       "      <th>Country_United Kingdom</th>\n",
       "      <th>Country</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Alpendorf (Ski amedé)</td>\n",
       "      <td>1980.0</td>\n",
       "      <td>740.0</td>\n",
       "      <td>52.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>81</td>\n",
       "      <td>4.0</td>\n",
       "      <td>115</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Austria</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Soldeu-Pas de la Casa/​Grau Roig/​El Tarter/​C...</td>\n",
       "      <td>2640.0</td>\n",
       "      <td>1710.0</td>\n",
       "      <td>47.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>77</td>\n",
       "      <td>33.0</td>\n",
       "      <td>210</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Andorra</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Oberau (Wildschönau)</td>\n",
       "      <td>1130.0</td>\n",
       "      <td>900.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Austria</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Dachstein West</td>\n",
       "      <td>1620.0</td>\n",
       "      <td>780.0</td>\n",
       "      <td>42.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>33</td>\n",
       "      <td>3.0</td>\n",
       "      <td>51</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Austria</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Rosa Khutor</td>\n",
       "      <td>2320.0</td>\n",
       "      <td>940.0</td>\n",
       "      <td>22.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>26</td>\n",
       "      <td>21.0</td>\n",
       "      <td>77</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Southern Russia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>372</th>\n",
       "      <td>Montgenèvre (Via Lattea)</td>\n",
       "      <td>2749.0</td>\n",
       "      <td>1372.0</td>\n",
       "      <td>48.0</td>\n",
       "      <td>96.0</td>\n",
       "      <td>220</td>\n",
       "      <td>84.0</td>\n",
       "      <td>400</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>France</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>373</th>\n",
       "      <td>Sauze d’Oulx (Via Lattea)</td>\n",
       "      <td>2749.0</td>\n",
       "      <td>1372.0</td>\n",
       "      <td>48.0</td>\n",
       "      <td>96.0</td>\n",
       "      <td>220</td>\n",
       "      <td>84.0</td>\n",
       "      <td>400</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Italy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>374</th>\n",
       "      <td>Gressoney - La-Trinite (Monterosa Ski)</td>\n",
       "      <td>3275.0</td>\n",
       "      <td>1212.0</td>\n",
       "      <td>43.0</td>\n",
       "      <td>23.0</td>\n",
       "      <td>94</td>\n",
       "      <td>15.0</td>\n",
       "      <td>132</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Italy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>375</th>\n",
       "      <td>Champoluc (Monterosa Ski)</td>\n",
       "      <td>3275.0</td>\n",
       "      <td>1212.0</td>\n",
       "      <td>43.0</td>\n",
       "      <td>23.0</td>\n",
       "      <td>94</td>\n",
       "      <td>15.0</td>\n",
       "      <td>132</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Italy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>376</th>\n",
       "      <td>Zauchensee</td>\n",
       "      <td>2188.0</td>\n",
       "      <td>1000.0</td>\n",
       "      <td>52.0</td>\n",
       "      <td>23.0</td>\n",
       "      <td>16</td>\n",
       "      <td>4.0</td>\n",
       "      <td>44</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Austria</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>376 rows × 44 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                Resort  HighestPoint  \\\n",
       "1                                Alpendorf (Ski amedé)        1980.0   \n",
       "2    Soldeu-Pas de la Casa/​Grau Roig/​El Tarter/​C...        2640.0   \n",
       "3                                Oberau (Wildschönau)        1130.0   \n",
       "4                                       Dachstein West        1620.0   \n",
       "5                                          Rosa Khutor        2320.0   \n",
       "..                                                 ...           ...   \n",
       "372                          Montgenèvre (Via Lattea)        2749.0   \n",
       "373                          Sauze d’Oulx (Via Lattea)        2749.0   \n",
       "374             Gressoney - La-Trinite (Monterosa Ski)        3275.0   \n",
       "375                          Champoluc (Monterosa Ski)        3275.0   \n",
       "376                                         Zauchensee        2188.0   \n",
       "\n",
       "     LowestPoint  DayPassPriceAdult  BeginnerSlope  IntermediateSlope  \\\n",
       "1          740.0               52.0           30.0                 81   \n",
       "2         1710.0               47.0          100.0                 77   \n",
       "3          900.0               30.0            1.0                  0   \n",
       "4          780.0               42.0           15.0                 33   \n",
       "5          940.0               22.0           30.0                 26   \n",
       "..           ...                ...            ...                ...   \n",
       "372       1372.0               48.0           96.0                220   \n",
       "373       1372.0               48.0           96.0                220   \n",
       "374       1212.0               43.0           23.0                 94   \n",
       "375       1212.0               43.0           23.0                 94   \n",
       "376       1000.0               52.0           23.0                 16   \n",
       "\n",
       "     DifficultSlope  TotalSlope  Snowparks  NightSki  ...  Country_Siberia  \\\n",
       "1               4.0         115       True     False  ...                0   \n",
       "2              33.0         210       True      True  ...                0   \n",
       "3               1.0           2      False     False  ...                0   \n",
       "4               3.0          51       True      True  ...                0   \n",
       "5              21.0          77       True     False  ...                0   \n",
       "..              ...         ...        ...       ...  ...              ...   \n",
       "372            84.0         400      False      True  ...                0   \n",
       "373            84.0         400      False      True  ...                0   \n",
       "374            15.0         132       True     False  ...                0   \n",
       "375            15.0         132       True     False  ...                0   \n",
       "376             4.0          44       True     False  ...                0   \n",
       "\n",
       "     Country_Slovakia  Country_Slovenia  Country_Southern Russia  \\\n",
       "1                   0                 0                        0   \n",
       "2                   0                 0                        0   \n",
       "3                   0                 0                        0   \n",
       "4                   0                 0                        0   \n",
       "5                   0                 0                        1   \n",
       "..                ...               ...                      ...   \n",
       "372                 0                 0                        0   \n",
       "373                 0                 0                        0   \n",
       "374                 0                 0                        0   \n",
       "375                 0                 0                        0   \n",
       "376                 0                 0                        0   \n",
       "\n",
       "     Country_Spain  Country_Sweden  Country_Switzerland  Country_Ukraine  \\\n",
       "1                0               0                    0                0   \n",
       "2                0               0                    0                0   \n",
       "3                0               0                    0                0   \n",
       "4                0               0                    0                0   \n",
       "5                0               0                    0                0   \n",
       "..             ...             ...                  ...              ...   \n",
       "372              0               0                    0                0   \n",
       "373              0               0                    0                0   \n",
       "374              0               0                    0                0   \n",
       "375              0               0                    0                0   \n",
       "376              0               0                    0                0   \n",
       "\n",
       "     Country_United Kingdom          Country  \n",
       "1                         0          Austria  \n",
       "2                         0          Andorra  \n",
       "3                         0          Austria  \n",
       "4                         0          Austria  \n",
       "5                         0  Southern Russia  \n",
       "..                      ...              ...  \n",
       "372                       0           France  \n",
       "373                       0            Italy  \n",
       "374                       0            Italy  \n",
       "375                       0            Italy  \n",
       "376                       0          Austria  \n",
       "\n",
       "[376 rows x 44 columns]"
      ]
     },
     "execution_count": 177,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# your code for 2. Transform the columns \"Snowparks\" and \"NightSki\" in `df` into boolean data type, where \"Yes\" should be `True` and \"No\" should be `False`\n",
    "to_bool = ['Snowparks', 'NightSki']\n",
    "for col in to_bool:\n",
    "    df2[col] = (df2[col] == 'Yes')\n",
    "df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "id": "02638dbf-36cd-47d8-9eee-07591eeb5ed9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Resort                              object\n",
       "HighestPoint                       float64\n",
       "LowestPoint                        float64\n",
       "DayPassPriceAdult                  float64\n",
       "BeginnerSlope                      float64\n",
       "IntermediateSlope                    int64\n",
       "DifficultSlope                     float64\n",
       "TotalSlope                           int64\n",
       "Snowparks                             bool\n",
       "NightSki                              bool\n",
       "SurfaceLifts                       float64\n",
       "ChairLifts                           int64\n",
       "GondolaLifts                       float64\n",
       "TotalLifts                           int64\n",
       "LiftCapacity                       float64\n",
       "SnowCannons                        float64\n",
       "Country_Andorra                      uint8\n",
       "Country_Austria                      uint8\n",
       "Country_Bosnia and Herzegovina       uint8\n",
       "Country_Bulgaria                     uint8\n",
       "Country_Czech Republic               uint8\n",
       "Country_Denmark                      uint8\n",
       "Country_Finland                      uint8\n",
       "Country_France                       uint8\n",
       "Country_Germany                      uint8\n",
       "Country_Greece                       uint8\n",
       "Country_Italy                        uint8\n",
       "Country_Liechtenstein                uint8\n",
       "Country_Lithuania                    uint8\n",
       "Country_Netherlands                  uint8\n",
       "Country_Norway                       uint8\n",
       "Country_Poland                       uint8\n",
       "Country_Romania                      uint8\n",
       "Country_Serbia                       uint8\n",
       "Country_Siberia                      uint8\n",
       "Country_Slovakia                     uint8\n",
       "Country_Slovenia                     uint8\n",
       "Country_Southern Russia              uint8\n",
       "Country_Spain                        uint8\n",
       "Country_Sweden                       uint8\n",
       "Country_Switzerland                  uint8\n",
       "Country_Ukraine                      uint8\n",
       "Country_United Kingdom               uint8\n",
       "Country                           category\n",
       "dtype: object"
      ]
     },
     "execution_count": 178,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# use df.dtypes to check if you correctly transform the data\n",
    "df2.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "id": "132575eb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Resort</th>\n",
       "      <th>HighestPoint</th>\n",
       "      <th>LowestPoint</th>\n",
       "      <th>DayPassPriceAdult</th>\n",
       "      <th>BeginnerSlope</th>\n",
       "      <th>IntermediateSlope</th>\n",
       "      <th>DifficultSlope</th>\n",
       "      <th>TotalSlope</th>\n",
       "      <th>Snowparks</th>\n",
       "      <th>NightSki</th>\n",
       "      <th>...</th>\n",
       "      <th>Country_Siberia</th>\n",
       "      <th>Country_Slovakia</th>\n",
       "      <th>Country_Slovenia</th>\n",
       "      <th>Country_Southern Russia</th>\n",
       "      <th>Country_Spain</th>\n",
       "      <th>Country_Sweden</th>\n",
       "      <th>Country_Switzerland</th>\n",
       "      <th>Country_Ukraine</th>\n",
       "      <th>Country_United Kingdom</th>\n",
       "      <th>Country</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Alpendorf (Ski amedé)</td>\n",
       "      <td>1980.0</td>\n",
       "      <td>740.0</td>\n",
       "      <td>52.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>81</td>\n",
       "      <td>4.0</td>\n",
       "      <td>115</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Austria</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Soldeu-Pas de la Casa/​Grau Roig/​El Tarter/​C...</td>\n",
       "      <td>2640.0</td>\n",
       "      <td>1710.0</td>\n",
       "      <td>47.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>77</td>\n",
       "      <td>33.0</td>\n",
       "      <td>210</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Andorra</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Oberau (Wildschönau)</td>\n",
       "      <td>1130.0</td>\n",
       "      <td>900.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Austria</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Dachstein West</td>\n",
       "      <td>1620.0</td>\n",
       "      <td>780.0</td>\n",
       "      <td>42.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>33</td>\n",
       "      <td>3.0</td>\n",
       "      <td>51</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Austria</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Rosa Khutor</td>\n",
       "      <td>2320.0</td>\n",
       "      <td>940.0</td>\n",
       "      <td>22.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>26</td>\n",
       "      <td>21.0</td>\n",
       "      <td>77</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Southern Russia</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 44 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              Resort  HighestPoint  \\\n",
       "1                              Alpendorf (Ski amedé)        1980.0   \n",
       "2  Soldeu-Pas de la Casa/​Grau Roig/​El Tarter/​C...        2640.0   \n",
       "3                              Oberau (Wildschönau)        1130.0   \n",
       "4                                     Dachstein West        1620.0   \n",
       "5                                        Rosa Khutor        2320.0   \n",
       "\n",
       "   LowestPoint  DayPassPriceAdult  BeginnerSlope  IntermediateSlope  \\\n",
       "1        740.0               52.0           30.0                 81   \n",
       "2       1710.0               47.0          100.0                 77   \n",
       "3        900.0               30.0            1.0                  0   \n",
       "4        780.0               42.0           15.0                 33   \n",
       "5        940.0               22.0           30.0                 26   \n",
       "\n",
       "   DifficultSlope  TotalSlope  Snowparks  NightSki  ...  Country_Siberia  \\\n",
       "1             4.0         115       True     False  ...                0   \n",
       "2            33.0         210       True      True  ...                0   \n",
       "3             1.0           2      False     False  ...                0   \n",
       "4             3.0          51       True      True  ...                0   \n",
       "5            21.0          77       True     False  ...                0   \n",
       "\n",
       "   Country_Slovakia  Country_Slovenia  Country_Southern Russia  Country_Spain  \\\n",
       "1                 0                 0                        0              0   \n",
       "2                 0                 0                        0              0   \n",
       "3                 0                 0                        0              0   \n",
       "4                 0                 0                        0              0   \n",
       "5                 0                 0                        1              0   \n",
       "\n",
       "   Country_Sweden  Country_Switzerland  Country_Ukraine  \\\n",
       "1               0                    0                0   \n",
       "2               0                    0                0   \n",
       "3               0                    0                0   \n",
       "4               0                    0                0   \n",
       "5               0                    0                0   \n",
       "\n",
       "   Country_United Kingdom          Country  \n",
       "1                       0          Austria  \n",
       "2                       0          Andorra  \n",
       "3                       0          Austria  \n",
       "4                       0          Austria  \n",
       "5                       0  Southern Russia  \n",
       "\n",
       "[5 rows x 44 columns]"
      ]
     },
     "execution_count": 179,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# First 5 rows\n",
    "df2.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c3192a2-dedb-45ab-b5c0-86b49a7a15e9",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### **b(iii)** \n",
    "The previous analysis in b(i) showed that there are missing values in the 'SurfaceLifts' and 'GondolaLifts' columns.\\\n",
    "How would you impute these values? \\\n",
    "Motivate your approach and apply it to `df`.\n",
    "\n",
    "*Hint: Remember the semantics of the columns. Also, carefully assert your assumptions.*\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0e34370-d15a-4f57-8df1-b11ae3e4b9a5",
   "metadata": {},
   "source": [
    "**Your Answer:** *(Motivate your approach.)* From the description, TotalLifts is the sum of SurfaceLifts + GondolaLifts + ChairLifts. Luckily, the code below shows that there is no row where both the values of GondolaLifts and SurfaceLifts are missing. Also from exercise **b(i)**, there is no missing value in ChairLifts. So we can impute these as follows:\n",
    "- Where GondolaLifts is missing, impute it with TotalLifts - SurfaceLifts - ChairLifts.\n",
    "- Where SurfaceLifts is missing, impute it with TotalLifts - GondolaLifts - ChairLifts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "id": "07628a31-b88b-4692-92ca-2aec93f36173",
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code\n",
    "gondola = df2[df2['GondolaLifts'].isna()].copy()\n",
    "gondola['GondolaLifts'] = gondola['TotalLifts'] - gondola['ChairLifts'] - gondola['SurfaceLifts']\n",
    "surface = df2[df2['SurfaceLifts'].isna()].copy()\n",
    "surface['SurfaceLifts'] = surface['TotalLifts'] - surface['ChairLifts'] - surface['GondolaLifts']\n",
    "df2.loc[gondola.index, ['GondolaLifts']] = gondola['GondolaLifts']\n",
    "df2.loc[surface.index, ['SurfaceLifts']] = surface['SurfaceLifts']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a15869a-3c62-4363-9248-012b1ae373d0",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### **b(iv)**\n",
    "Impute the rest of the missing values using the knn-imputation method. To this end, apply the following steps:\n",
    "1. Create a working copy `df_tmp` of your updated `df`.\n",
    "2. For simplicity, drop the non-numerical columns (i.e., not of types integer or floats), which also inlcude the one-hot encoded and the boolean columns* you created earlier.\n",
    "3. Normalize the data in `df_tmp` (e.g., Standard score normalization). If the features have very different scales, knn can become very biased.\n",
    "4. Impute the missing values considering six neighbors.\n",
    "5. Invert the transformation applied upfront to enable more meaningful and intuitive visualizations.\n",
    "6. Append the columns you dropped in step 2.\n",
    " \n",
    "In the end, `df` should not contain missing values and have all the columns.\n",
    "\n",
    "\\*Note that by dropping the columns we lose the information of countries and the two boolean attributes (\"Snowparks\" and \"NightSki\") when imputing the missing values, which might be crucial for inferencing values such as the price for a ski pass. In practice, one should try to find if there are correlations before deciding whether to drop the columns or not.\n",
    "We drop the columns here to make the following steps easier because we only have to deal with numerical columns.\n",
    "\n",
    "*Hint: Be careful with the indices of your dataframes.*\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "id": "e1dc58ba-2699-4826-9dda-f0c5c135f32e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['HighestPoint', 'LowestPoint', 'DayPassPriceAdult', 'BeginnerSlope',\n",
       "       'IntermediateSlope', 'DifficultSlope', 'TotalSlope', 'SurfaceLifts',\n",
       "       'ChairLifts', 'GondolaLifts', 'TotalLifts', 'LiftCapacity',\n",
       "       'SnowCannons'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 181,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# your code for step 1 & 2\n",
    "df_tmp = df2.copy()\n",
    "df_tmp = df_tmp[df.columns]\n",
    "dropped_cols = ['Country', 'Resort', 'NightSki', 'Snowparks']\n",
    "df_tmp = df_tmp.drop(dropped_cols, axis=1)\n",
    "df_tmp.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "id": "7600aaaf-a864-47bb-aa62-76bfd644fd53",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.49192481, 0.33944954, 0.64197531, ..., 0.2816092 , 0.29886634,\n",
       "        0.25178347],\n",
       "       [0.66666667, 0.78440367, 0.58024691, ..., 0.4137931 , 0.3924885 ,\n",
       "        0.43306756],\n",
       "       [0.26687847, 0.41284404, 0.37037037, ..., 0.01149425, 0.00765816,\n",
       "        0.        ],\n",
       "       ...,\n",
       "       [0.83478952, 0.5559633 , 0.5308642 , ..., 0.17241379, 0.12677977,\n",
       "        0.27486362],\n",
       "       [0.83478952, 0.5559633 , 0.5308642 , ..., 0.17241379, 0.12677977,\n",
       "        0.27486362],\n",
       "       [0.54699497, 0.4587156 , 0.64197531, ..., 0.1091954 , 0.10301253,\n",
       "        0.04741922]])"
      ]
     },
     "execution_count": 182,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# your code for step 3\n",
    "scaler = MinMaxScaler()\n",
    "df_scaled = scaler.fit_transform(df_tmp)\n",
    "df_scaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "id": "1ce24019-3dba-438d-a7ea-dcc3244bbc53",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.49192481, 0.33944954, 0.64197531, ..., 0.2816092 , 0.29886634,\n",
       "        0.25178347],\n",
       "       [0.66666667, 0.78440367, 0.58024691, ..., 0.4137931 , 0.3924885 ,\n",
       "        0.43306756],\n",
       "       [0.26687847, 0.41284404, 0.37037037, ..., 0.01149425, 0.00765816,\n",
       "        0.        ],\n",
       "       ...,\n",
       "       [0.83478952, 0.5559633 , 0.5308642 , ..., 0.17241379, 0.12677977,\n",
       "        0.27486362],\n",
       "       [0.83478952, 0.5559633 , 0.5308642 , ..., 0.17241379, 0.12677977,\n",
       "        0.27486362],\n",
       "       [0.54699497, 0.4587156 , 0.64197531, ..., 0.1091954 , 0.10301253,\n",
       "        0.04741922]])"
      ]
     },
     "execution_count": 183,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# your code for step 4\n",
    "imputer = KNNImputer(n_neighbors=6)\n",
    "df_scaled_imputed = imputer.fit_transform(df_scaled)\n",
    "df_scaled_imputed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "id": "42034302-c720-4f4d-9512-10bfa9842d70",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.9800e+03, 7.4000e+02, 5.2000e+01, ..., 4.9000e+01, 7.5398e+04,\n",
       "        6.0000e+02],\n",
       "       [2.6400e+03, 1.7100e+03, 4.7000e+01, ..., 7.2000e+01, 9.9017e+04,\n",
       "        1.0320e+03],\n",
       "       [1.1300e+03, 9.0000e+02, 3.0000e+01, ..., 2.0000e+00, 1.9320e+03,\n",
       "        0.0000e+00],\n",
       "       ...,\n",
       "       [3.2750e+03, 1.2120e+03, 4.3000e+01, ..., 3.0000e+01, 3.1984e+04,\n",
       "        6.5500e+02],\n",
       "       [3.2750e+03, 1.2120e+03, 4.3000e+01, ..., 3.0000e+01, 3.1984e+04,\n",
       "        6.5500e+02],\n",
       "       [2.1880e+03, 1.0000e+03, 5.2000e+01, ..., 1.9000e+01, 2.5988e+04,\n",
       "        1.1300e+02]])"
      ]
     },
     "execution_count": 184,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# your code for step 5\n",
    "df_scaled_imputed_inverted = scaler.inverse_transform(df_scaled_imputed)\n",
    "df_scaled_imputed_inverted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "id": "6de8f607-4016-4a8c-966e-0bcd3ee086b9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>HighestPoint</th>\n",
       "      <th>LowestPoint</th>\n",
       "      <th>DayPassPriceAdult</th>\n",
       "      <th>BeginnerSlope</th>\n",
       "      <th>IntermediateSlope</th>\n",
       "      <th>DifficultSlope</th>\n",
       "      <th>TotalSlope</th>\n",
       "      <th>SurfaceLifts</th>\n",
       "      <th>ChairLifts</th>\n",
       "      <th>GondolaLifts</th>\n",
       "      <th>...</th>\n",
       "      <th>Country_Southern Russia</th>\n",
       "      <th>Country_Spain</th>\n",
       "      <th>Country_Sweden</th>\n",
       "      <th>Country_Switzerland</th>\n",
       "      <th>Country_Ukraine</th>\n",
       "      <th>Country_United Kingdom</th>\n",
       "      <th>Country</th>\n",
       "      <th>Resort</th>\n",
       "      <th>NightSki</th>\n",
       "      <th>Snowparks</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1980.0</td>\n",
       "      <td>740.0</td>\n",
       "      <td>52.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>81.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>115.0</td>\n",
       "      <td>22.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Austria</td>\n",
       "      <td>Alpendorf (Ski amedé)</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2640.0</td>\n",
       "      <td>1710.0</td>\n",
       "      <td>47.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>77.0</td>\n",
       "      <td>33.0</td>\n",
       "      <td>210.0</td>\n",
       "      <td>37.0</td>\n",
       "      <td>28.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Andorra</td>\n",
       "      <td>Soldeu-Pas de la Casa/​Grau Roig/​El Tarter/​C...</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1130.0</td>\n",
       "      <td>900.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Austria</td>\n",
       "      <td>Oberau (Wildschönau)</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1620.0</td>\n",
       "      <td>780.0</td>\n",
       "      <td>42.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>33.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>51.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Austria</td>\n",
       "      <td>Dachstein West</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2320.0</td>\n",
       "      <td>940.0</td>\n",
       "      <td>22.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>26.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>77.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Southern Russia</td>\n",
       "      <td>Rosa Khutor</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>372</th>\n",
       "      <td>2749.0</td>\n",
       "      <td>1372.0</td>\n",
       "      <td>48.0</td>\n",
       "      <td>96.0</td>\n",
       "      <td>220.0</td>\n",
       "      <td>84.0</td>\n",
       "      <td>400.0</td>\n",
       "      <td>29.0</td>\n",
       "      <td>35.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>France</td>\n",
       "      <td>Montgenèvre (Via Lattea)</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>373</th>\n",
       "      <td>2749.0</td>\n",
       "      <td>1372.0</td>\n",
       "      <td>48.0</td>\n",
       "      <td>96.0</td>\n",
       "      <td>220.0</td>\n",
       "      <td>84.0</td>\n",
       "      <td>400.0</td>\n",
       "      <td>29.0</td>\n",
       "      <td>35.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Italy</td>\n",
       "      <td>Sauze d’Oulx (Via Lattea)</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>374</th>\n",
       "      <td>3275.0</td>\n",
       "      <td>1212.0</td>\n",
       "      <td>43.0</td>\n",
       "      <td>23.0</td>\n",
       "      <td>94.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>132.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Italy</td>\n",
       "      <td>Gressoney - La-Trinite (Monterosa Ski)</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>375</th>\n",
       "      <td>3275.0</td>\n",
       "      <td>1212.0</td>\n",
       "      <td>43.0</td>\n",
       "      <td>23.0</td>\n",
       "      <td>94.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>132.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Italy</td>\n",
       "      <td>Champoluc (Monterosa Ski)</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>376</th>\n",
       "      <td>2188.0</td>\n",
       "      <td>1000.0</td>\n",
       "      <td>52.0</td>\n",
       "      <td>23.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>44.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Austria</td>\n",
       "      <td>Zauchensee</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>376 rows × 44 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     HighestPoint  LowestPoint  DayPassPriceAdult  BeginnerSlope  \\\n",
       "1          1980.0        740.0               52.0           30.0   \n",
       "2          2640.0       1710.0               47.0          100.0   \n",
       "3          1130.0        900.0               30.0            1.0   \n",
       "4          1620.0        780.0               42.0           15.0   \n",
       "5          2320.0        940.0               22.0           30.0   \n",
       "..            ...          ...                ...            ...   \n",
       "372        2749.0       1372.0               48.0           96.0   \n",
       "373        2749.0       1372.0               48.0           96.0   \n",
       "374        3275.0       1212.0               43.0           23.0   \n",
       "375        3275.0       1212.0               43.0           23.0   \n",
       "376        2188.0       1000.0               52.0           23.0   \n",
       "\n",
       "     IntermediateSlope  DifficultSlope  TotalSlope  SurfaceLifts  ChairLifts  \\\n",
       "1                 81.0             4.0       115.0          22.0        16.0   \n",
       "2                 77.0            33.0       210.0          37.0        28.0   \n",
       "3                  0.0             1.0         2.0           2.0         0.0   \n",
       "4                 33.0             3.0        51.0          25.0         8.0   \n",
       "5                 26.0            21.0        77.0           6.0        11.0   \n",
       "..                 ...             ...         ...           ...         ...   \n",
       "372              220.0            84.0       400.0          29.0        35.0   \n",
       "373              220.0            84.0       400.0          29.0        35.0   \n",
       "374               94.0            15.0       132.0           9.0         9.0   \n",
       "375               94.0            15.0       132.0           9.0         9.0   \n",
       "376               16.0             4.0        44.0           9.0         6.0   \n",
       "\n",
       "     GondolaLifts  ...  Country_Southern Russia  Country_Spain  \\\n",
       "1            11.0  ...                        0              0   \n",
       "2             7.0  ...                        0              0   \n",
       "3             0.0  ...                        0              0   \n",
       "4             3.0  ...                        0              0   \n",
       "5            10.0  ...                        1              0   \n",
       "..            ...  ...                      ...            ...   \n",
       "372           7.0  ...                        0              0   \n",
       "373           7.0  ...                        0              0   \n",
       "374          12.0  ...                        0              0   \n",
       "375          12.0  ...                        0              0   \n",
       "376           4.0  ...                        0              0   \n",
       "\n",
       "     Country_Sweden  Country_Switzerland  Country_Ukraine  \\\n",
       "1                 0                    0                0   \n",
       "2                 0                    0                0   \n",
       "3                 0                    0                0   \n",
       "4                 0                    0                0   \n",
       "5                 0                    0                0   \n",
       "..              ...                  ...              ...   \n",
       "372               0                    0                0   \n",
       "373               0                    0                0   \n",
       "374               0                    0                0   \n",
       "375               0                    0                0   \n",
       "376               0                    0                0   \n",
       "\n",
       "     Country_United Kingdom          Country  \\\n",
       "1                         0          Austria   \n",
       "2                         0          Andorra   \n",
       "3                         0          Austria   \n",
       "4                         0          Austria   \n",
       "5                         0  Southern Russia   \n",
       "..                      ...              ...   \n",
       "372                       0           France   \n",
       "373                       0            Italy   \n",
       "374                       0            Italy   \n",
       "375                       0            Italy   \n",
       "376                       0          Austria   \n",
       "\n",
       "                                                Resort  NightSki  Snowparks  \n",
       "1                                Alpendorf (Ski amedé)     False       True  \n",
       "2    Soldeu-Pas de la Casa/​Grau Roig/​El Tarter/​C...      True       True  \n",
       "3                                Oberau (Wildschönau)     False      False  \n",
       "4                                       Dachstein West      True       True  \n",
       "5                                          Rosa Khutor     False       True  \n",
       "..                                                 ...       ...        ...  \n",
       "372                          Montgenèvre (Via Lattea)      True      False  \n",
       "373                          Sauze d’Oulx (Via Lattea)      True      False  \n",
       "374             Gressoney - La-Trinite (Monterosa Ski)     False       True  \n",
       "375                          Champoluc (Monterosa Ski)     False       True  \n",
       "376                                         Zauchensee     False       True  \n",
       "\n",
       "[376 rows x 44 columns]"
      ]
     },
     "execution_count": 190,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# your code for step 6\n",
    "# To make rerunning cells not a pain, we store the final result in a new Dataframe dff\n",
    "dff = df_tmp.copy()\n",
    "dff.loc[:,scaler.feature_names_in_] = df_scaled_imputed_inverted\n",
    "dff = pd.concat([dff, df2[df2.filter(regex=(\"Country_*\")).columns]], axis=1) # Reappend the one-hot columns\n",
    "dff[dropped_cols] = df2[dropped_cols] # Reappend the dropped categorical columns \n",
    "dff # Final result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "id": "7a5dd094-f496-4a98-81fb-1e941ee90472",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 193,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# assert df.isna().sum().sum() == 0\n",
    "assert dff.isna().sum().sum() == 0\n",
    "dff.isna().sum().sum() # To believe it, we have to see it"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52685085-a3d3-4a40-a979-069ec578c01e",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Question 2: Visualization (13 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08fdcbdb-792d-4c66-b5ef-4e86d6a955b5",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "In this task, you will analyze the data using different means of visualization.\n",
    "\n",
    "Start with the following preprocessed and integrated dataframe `df_v`. \\\n",
    "Note that it has a similar structure to the dataframe that you should obtain from the previous task, however, the values have been modified."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "620155fc-f0c9-4f49-b778-6379c949bb2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import plotly.express as px\n",
    "from pandas.plotting import scatter_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "570ecd50-427b-4203-bb2f-f2fcbd1052a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_v = pd.read_csv(\"./datasets/ski_resorts_visual.csv\", index_col=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9de753a8-98a9-4336-9e91-0acc1c73cf60",
   "metadata": {
    "tags": []
   },
   "source": [
    "### **a) (3pts)** \n",
    "To start the visual analysis, make a Scatter plot matrix to visually check if there are any correlations between the numerical attributes.\n",
    "\n",
    "*Hint: You can use the scatter_matrix from pandas.plotting or pairplot from seaborn to make the plot.*\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12a7b99e-a13f-42b0-862b-91968efe7542",
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d60f31f7-cba3-4c6d-be6f-fdefb5c6fb17",
   "metadata": {
    "tags": []
   },
   "source": [
    "### **b) (3pts)**\n",
    "Another way to detect correlation is to calculate the Pearson correlation coefficient. Calculate the correlation matrix for the numerical data and visualize the matrix using a heatmap. \n",
    "Briefly discuss your findings from the heatmap and the scatter plot you created in 2(a).\n",
    "\n",
    "Make sure to annotate the heatmap with the values of the correlation.\n",
    "\n",
    "*Hint: You can use the heatmap function from seaborn to make the plot.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bd3d2ce-7a30-4fa6-a5e9-46cefd79e26f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f387c1b8-4e34-4b68-88bf-774048366104",
   "metadata": {
    "tags": []
   },
   "source": [
    "### **c) (4pts)** \n",
    "We now want to have an overview of the attribute \"TotalSlope\" aggregated by different levels of hierarchy (Europe -> Country -> Resort). It seems that a tree map is suitable for this purpose.\n",
    "\n",
    "Make a tree map where\n",
    "- the root node represents Europe.\n",
    "- the child nodes of Europe are countries.\n",
    "- the child nodes of each country are the ski resorts.\n",
    "- the size of the rectangles is determined by the attribute \"TotalSlope\".\n",
    "\n",
    "Also, use the tree map to find out\n",
    "1. The sum of TotalSlopes of a country, list the top five countries and the corresponding values.\n",
    "2. The max value of TotalSlope of the five countries you identified in 1.\n",
    "\n",
    "*Hint: You can use the treemap function from plotly.express.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eed294b7-8822-45fe-adfc-4a4e4d024124",
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6279cedc-ab47-4bc1-ac36-bdac4fb91f2b",
   "metadata": {},
   "source": [
    "**Your answer for...** \\\n",
    "*...  1. The sum of TotalSlopes of a country. List the top five countries and the corresponding values:* \\\n",
    "*...  2. The max value of TotalSlope of the five countries you identified in 1:*\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cd91227-df95-4371-8538-fac931bc4745",
   "metadata": {
    "tags": []
   },
   "source": [
    "### **d) (3pts)** \n",
    "\n",
    "An alternative to a tree map is a sunburst plot, the principle is similar to a tree map. \n",
    "\n",
    "Recall from the lecture that:\n",
    "- Each ring is a different level of the hierarchy\n",
    "- Each segment of a ring belongs to one categorical value\n",
    "- The size of a segment is either divided proportionally to a value\n",
    "\n",
    "Now, we would like to have an overview of the attribute \"TotalLifts\" aggregated by different level of hierarchy.\n",
    "\n",
    "Make a sunburst plot where\n",
    "- the first hierarchy(ring) is \"Country\"\n",
    "- the second hierarchy(ring) is \"Snowparks\" (whether the resort has snowparks)\n",
    "- the third hierarchy(ring) is \"Resort\"\n",
    "- the size of the segments is determined by the attribute \"TotalLifts\".\n",
    "\n",
    "Then, briefly discuss your findings from the plot.\n",
    "\n",
    "*Hint: You can use the sunburst function from plotly.express.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7203fa15-8f8b-4cb7-b836-de047bb6a2bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e24702c9-93a5-47d9-88ef-cbd018ae840b",
   "metadata": {},
   "source": [
    "**Your answer:**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62eb1982-1fa7-4829-9dfa-df0922f8de3c",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Question 3 - Frequent Item Sets and Association Rules (15 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc7d780a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlxtend.preprocessing import TransactionEncoder\n",
    "from mlxtend.frequent_patterns import apriori\n",
    "from mlxtend.frequent_patterns import association_rules as arule"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2e7e462-5ead-4531-a306-7bf6047647d5",
   "metadata": {
    "tags": []
   },
   "source": [
    "### A real online retail transaction data set of two years.\n",
    "\n",
    "Data Set Information:\n",
    "This Retail dataset contains all the transactions occurring for a UK-based and registered, non-store online retail between 01/12/2009 and 09/12/2011.The company mainly sells unique all-occasion gift ware. Many customers of the company are wholesalers.\n",
    "\n",
    "Attribute Information:\n",
    "- Invoice: Invoice number. Nominal. A 6-digit integral number is uniquely assigned to each transaction. If the number starts with 'C' it refers to a canceled transaction.\n",
    "- StockCode: Product (item) code. Nominal. A 5-digit integral number uniquely assigned to each distinct product.\n",
    "- Description: Product (item) name. Nominal.\n",
    "- Quantity: The quantities of each product (item) per transaction. Numeric.\n",
    "- InvoiceDate: Invoice date and time. Numeric. The day and time when a transaction was generated.\n",
    "- Price: Unit price. Numeric. Product price per unit in sterling (£).\n",
    "- CustomerID: Customer number. Nominal. A 5-digit integral number is uniquely assigned to each customer. This number has postfix 'n'.\n",
    "- Country: Country name. Nominal. The name of the country where a customer resides."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9794fa4d-e5c8-4699-be1b-389dbd32b9c4",
   "metadata": {},
   "source": [
    "### a) Loading, exploring and preprocessing the dataset (4 pts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27a2d2b7-4dda-4a52-a597-80fdfaae628f",
   "metadata": {},
   "source": [
    "#### **a(i)** \n",
    "Load the data from `retail.csv` and save it under the variable `retail_df`. Display the first few lines of the dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc75ad8d-88bb-4ef3-ab32-ae5a1f38954e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eba06313-e9f1-43cd-ab08-07e56458234b",
   "metadata": {},
   "source": [
    "#### **a(ii)** \n",
    "To get to know the dataset, do the following:\n",
    "\n",
    "- Show the number of rows in the dataset.\n",
    "- Show the number of unique customers.\n",
    "- Show the number of unique product names.\n",
    "- Show the number of unique invoices.\n",
    "- Show the number and the list of all the countries where the customers reside."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eadfbf62-1e6b-4b5d-9556-46d9c2fc2614",
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "071fc555-19fd-4488-8e2c-efdb0cb785be",
   "metadata": {},
   "source": [
    "#### **a(iii)** \n",
    "You are interested in analyzing itemsets that are frequently purchased together. Before continuing with that task, you have to make sure that the data are fit for such analysis. 1) More precisely, you want to make sure that there are no missing values in the data. 2) Moreover, you want to ensure that each item's name in the \"Description\" is consistent. E.g., you want \"Description\" values such as \" coffee black\", \"coffee &nbsp;black\", \" coffee black &nbsp;\", etc. to be mapped to the same value (e.g. \"coffee black\"). 3) Last but not least, you want to remove transactions that were canceled. Such transactions correspond to rows where the invoice number starts with letter 'C'.\n",
    "\n",
    "Apply these preprocessing steps to the dataset `retail_df` and apply them on the dataframe itself (e.g. set inplace=True)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bb891d0-6362-47c5-903d-4caf7333a2c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cdb88ce-11d8-405f-94b8-a0d670da853a",
   "metadata": {},
   "source": [
    "#### **a(iv)** \n",
    "After applying the preprocessing steps in **a(iii)** , repeat again the task **a(ii)**, that is:\n",
    "\n",
    "- Show the number of rows in the dataset.\n",
    "- Show the number of unique customers.\n",
    "- Show the number of all unique product names.\n",
    "- Show the list of all the countries where the customers reside.\n",
    "\n",
    "Which values changed?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ff1aab7-1048-4043-989b-cd97437f0ed8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51ace216-572c-47d9-977f-52c62d2cb26e",
   "metadata": {},
   "source": [
    "**Your answer:** "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "105a9af7-a3c1-4d19-8cd8-778276106c52",
   "metadata": {},
   "source": [
    "### b) Frequent itemsets and Association rules (8 pts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d55a65f-a5a3-4616-8365-d6a99f0b4386",
   "metadata": {},
   "source": [
    "#### **b(i)** \n",
    "Each invoice number in the dataset identifies a unique transaction. There are potentially many rows in the dataframe having the same invoice number. We want to analyze items that are frequently purchased together, that is, items that appear in the same transaction.\n",
    "\n",
    "Create a new dataframe named `transaction_df` with two columns: \"Invoice\" and \"Description\". Here the \"Invoice\" value is the index of the dataframe (the unique number identifying each row) and \"Description\" is the column containing all items (without duplicates) purchased within the transaction with that invoice number. Display the first few rows of your dataframe. How many rows does the `transaction_df` have?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec046573-e55d-4ea4-a9c6-7f9c82d8d5cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97f7615d-f7f2-4881-b081-23505c0875d6",
   "metadata": {},
   "source": [
    "**Your answer:**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8abcf0c1-0445-4701-874f-d7f8f600075f",
   "metadata": {},
   "source": [
    "#### **b(ii)** \n",
    "Next, we want to compute frequent itemsets and association rules based on the sets of items ordered together. Use the TransactionEncoder to transform `transaction_df` into a matrix such that the value in the i-th row and the j-th column is $True$ if the i-th itemset contains product j, and $False$ otherwise. Save the matrix into a dataframe named `transactions`. Display the shape of the matrix.\n",
    "\n",
    "*Hint: Note that your dataframe 'transactions' must contain as many rows as there are invoice numbers and as many columns as there are unique products.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5c0b0e0-46c9-4b8d-873a-1d0d1136ebdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a718591-452e-4d69-be6f-f2e53e2060c7",
   "metadata": {},
   "source": [
    "#### **b(iii)** \n",
    "Use the apriori method on `transactions` to obtain all frequent itemsets using min_support=0.01. Display all frequent itemsets that have at least three items. What support count does an itemset have for our case if it satisfies min_support=0.01?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1463627c-e2c3-43b0-b8c7-f1ad0591f9a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0ad89a8-70d2-4734-9780-c964e14fbf23",
   "metadata": {},
   "source": [
    "**Your answer:**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14eb37dd-da50-4b68-988d-23500611955b",
   "metadata": {},
   "source": [
    "#### **b(iv)**\n",
    "Now we will discover association rules from the frequent itemsets. Using only the frequent itemsets with min_support=0.01 (the ones obtained in **b(iii)**), generate different association rules using min_conf=0.6 and min_conf=0.9 as thresholds. Show the association rules for each of the thresholds. What do you notice w.r.t. the number of association rules produced for the different thresholds?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aba91aa5-f2ef-4e68-a70c-6f112c3b9ec6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47fca001-2e05-453a-970a-0155a1c83fbd",
   "metadata": {},
   "source": [
    "**Your answer:**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1920ddaf-f961-4c50-b34b-d5d338223675",
   "metadata": {},
   "source": [
    "#### **b(v)** \n",
    "From the association rules that satisfy the confidence threshold 0.6, select and show the two rules with the highest lift. What do you notice if you compare the two rules with each other?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4110f530-8c31-4be4-96f2-aaa5d4595130",
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "743f10f9-2067-4d68-9510-33a5699e4ae8",
   "metadata": {},
   "source": [
    "**Your answer:**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcc2b990-3a2d-409c-907a-de7a812aa687",
   "metadata": {},
   "source": [
    "#### **b(vi)** \n",
    "\n",
    "In the analysis tasks in **b)**, an itemset consisted of items that had the same invoice number (same transaction items). Thus, if an itemset was frequent, it meant that the items in it were frequently purchased together.\n",
    "An association rule $A \\Rightarrow B$ meant that if items in $A$ are purchased, then the items in $B$ are also purchased in that same transaction.\n",
    "\n",
    "Suppose that we would repeat the analysis, but this time, the itemsets would consist of items having the same \"CustomerID\" (bought from the same customer). Interpret the meaning of the frequent itemsets and association rules for this kind of itemsets."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7269908-782f-4900-a306-7507b7afd115",
   "metadata": {},
   "source": [
    "**Your answer:**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a438848-5297-4ca8-9e09-cf414903197b",
   "metadata": {},
   "source": [
    "### c) Sequence Mining (3 pts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71b2c05a-2fa5-4150-b5ce-895d9f1078cc",
   "metadata": {},
   "source": [
    "For this task, the dataset used is `retail_sequences.csv`. Run the cell below to save the dataset under the dataframe `retail_sequences`. Each row in the dataframe corresponds to a unique customer (from the retail dataset). The \"Customer\" column contains the customer ID, whereas the \"Sequence\" column contains the sequence of itemsets  purchased by that customer.  Each value of \"Sequence\" is a sequence (list) of itemsets $<I_1, I_2, ..., I_n>$. The items within the same itemset (list without duplicates) $I_i$ were purchased together (they had the same invoice number). The itemsets are ordered by the timestamp of the transaction (value of InvoiceDate)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5764854-1c5e-44a8-b83a-c1bed469e9fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "retail_sequences = pd.read_csv('datasets/retail_sequences.csv', converters={'Sequence': pd.eval})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6769f289-6619-41f4-855d-d5a690009e7b",
   "metadata": {
    "tags": []
   },
   "source": [
    "Given is the sequence *s= <{'lunch bag cars blue'}, {'herb marker rosemary','herb marker thyme'}, {'wooden star christmas scandinavian'}>*. Compute the support count of that sequence, that is, compute the number of customers whose corresponding itemset sequence contains it. Display its support count and the IDs of those customers.\n",
    "\n",
    "*Hint: In the dataset provided, all product names are unified. They are all lowercase and have no trailing spaces.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ff9fcd7-4dc8-48a9-8ae0-5c8f0e0d0f81",
   "metadata": {},
   "outputs": [],
   "source": [
    "s = [['lunch bag cars blue'], ['herb marker rosemary', 'herb marker thyme'] ,['wooden star christmas scandinavian']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "006414b7-f6eb-4ea6-8d76-9ac57408501b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bb34ba2-6c45-4dc7-97f1-0194de53b3f5",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Question 4: Text Mining (12 pts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abb4c2cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
    "from nltk.lm.preprocessing import padded_everygram_pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85c87d2b-a192-496d-ae0e-52865d498d72",
   "metadata": {},
   "source": [
    "### F.R.I.E.N.D.S."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "725586d9-d73b-40bd-938d-e62dcae05fee",
   "metadata": {},
   "source": [
    "In this task we will use the script from the well-known series \"F.R.I.E.N.D.S.\". We will apply feature extraction methods to map the line of each main character onto a vector of a vector space. Then we will train a classifier whose aim will be to predict the name of the character given a particular line from the script.\n",
    "In the end, we will train language models using N-grams and produce fake sentences for each of the main characters."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "945cdb03-6f30-4b39-93b5-11ae64b60c4e",
   "metadata": {},
   "source": [
    "### a) Data Loading and Preprocessing (4 pts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "172ce8b2-8381-4447-8c70-8a1f66fa4b17",
   "metadata": {},
   "source": [
    "#### **a(i)** \n",
    "Import the file `FRIENDS.csv` and save it into a dataframe named `friends_df`. Note that the dataframe must contain two columns: one indicating the character's name and one containing a line from the script. Display the first few lines from the dataframe.\n",
    "\n",
    "<i>FYI: The script has been filtered so that it only contains lines from the main characters. The order of the lines in the data is the same as the order of the lines in the original script. Metadata and scene descriptions have been removed. Your corpus consists of all the lines contained in the data. Each row's \"line\" value is a single document. </i>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d249dbeb-b7c9-48f3-8fb8-546d052daac1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf8ecf98-6140-4c73-9350-08060d26deee",
   "metadata": {},
   "source": [
    "#### **a(ii)**  \n",
    "Plot the line count distribution among the six main characters (the six possible values of the column \"character\"). For example, show a plot containing one bar for each character whose height reflects the number of lines in `friends_df`. Briefly comment on the plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "534f1199-6f25-4e00-83c7-8aefbbe75250",
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe800c4f-7a8b-4c45-9484-ffeedb161a2e",
   "metadata": {},
   "source": [
    "**Your answer:**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56c20766-da32-4e64-99af-1f8eb1a466e1",
   "metadata": {},
   "source": [
    "#### **a(iii)**  \n",
    "Create a corpus named `corpus` such that each document in the corpus corresponds to exactly one row's \"line\" in `friends_df`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a677e05f-51c6-431d-933f-961089327685",
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6a5e192-46e1-4c64-92d2-387c83e8d4c2",
   "metadata": {},
   "source": [
    "#### **a(iv)**  \n",
    "Write a function called `my_preprocessor` which, given a string, returns another string after tokenization, stopword removal and lemmatization have been applied. The remaining terms (tokens after stopword removal and lemmatization has been applied) should be joined in the same string using space ' '."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5365fc69-a32d-4b8a-a142-0ec4ae8aa988",
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b173fb1e-44ef-4e72-a087-2cc67d5464b1",
   "metadata": {},
   "source": [
    "#### **a(v)**   \n",
    "You must apply your preprocessor `my_preprocessor` on each line contained in the `corpus`. Create a preprocessed corpus named `corpus_p` which contains the same lines as `corpus` after the preprocessor `my_preprocessor` has been applied."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d46ad877-6f7d-45af-902a-313efaf6e9b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b89f6a33-d791-493d-b9a3-809cfad2d694",
   "metadata": {},
   "source": [
    "#### **a(vi)**   \n",
    "Split the `friends_df` dataset from the previous task into training (80%) and test (20%) data preserving the distribution based on the \"character\" value. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffae634d-d8d3-4057-84a8-aa4117cf73dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3d998c5-75fc-4673-bb69-2a4a644b8681",
   "metadata": {},
   "source": [
    "#### **a(vii)**   \n",
    "Similar to **a(iii)**, for the training data and the test data, create two corpora named `corpus_train` and `corpus_test` respectively. Each document in the training (test) corpus must correspond to exactly one row's \"line\" value in the corresponding training (test) dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab8fa13d-599d-4c60-b747-bde77c9e99d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b8945a0-a108-4380-b0fa-aec46768c284",
   "metadata": {},
   "source": [
    "### b) Set of Words (4.5 pts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a862af1a-83a6-46fd-ab0e-48a55d2667e9",
   "metadata": {},
   "source": [
    "#### **b(i)**  \n",
    "We want to encode our text in such a way that for each word in the vocabulary, we are only interested in whether the word appears or not in a given document. Create such a Set of Words encoding for the whole corpus `corpus`. Use the previously defined preprocessor `my_preprocessor` as preprocessor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "000b068a-d85d-405a-a6b2-b463d63afa70",
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e607c40e-1b8a-4bda-9dc2-0cc4f5872452",
   "metadata": {},
   "source": [
    "#### **b(ii)**  \n",
    "Pick one (any) of the lines of the \"line\" column in the `friends_df` dataset. Display the line in:\n",
    "    1) its original form, \n",
    "    2) its preprocessed version (the result contained in `corpus_p` after applying `my_preprocessor`), and \n",
    "    3) its encoding computed by the Set of Words method. This can be either an array (a vector) or a scipy matrix. \n",
    "Briefly comment on the Set of Words encoding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "152e71ca-9019-47b1-a827-b7227e85a20d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16b431ce-c9ef-46fd-80f9-5c0ee685770b",
   "metadata": {},
   "source": [
    "**Your answer:**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c704be66-78e8-40b5-9f88-3df1939000e6",
   "metadata": {},
   "source": [
    "#### **b(iii)**  \n",
    "Create a Set of Words encoding based only on the documents in `corpus_train`. Use the previously defined preprocessor `my_preprocessor` as a preprocessor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbedb7eb-192a-4f0d-b590-9cf3cf8a813d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d26cad99-701b-4e56-a020-5e2d1ef04a4d",
   "metadata": {},
   "source": [
    "#### **b(iv)**  \n",
    "In this task, we will use an SGD (Stochastic Gradient Descend) classifier to predict the character given a line from the corpus. Train the classifier on the Set of Words encoding of training corpus `corpus_train` using the character as the target feature and 'log_loss' as the loss function. Apply the classifier on the Set of Words encodings of both the training corpus and the test corpus `corpus_train`. Show its accuracy for both the training corpus and the test corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5aeffa1d-bb0b-4112-846c-74cde3e217dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c77974b1-4fb7-4fa9-9ef5-40764d4a0a28",
   "metadata": {},
   "source": [
    "#### **b(v)**  \n",
    "Briefly comment on the accuracy of the classifier compared to the expected accuracy of a random guesser (here: a model that simply guesses each character according to a distribution based on the line count). Use the line count distribution shown in **a(ii)** to reason about the approximate accuracy of the random guesser."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86b7d8d1-3375-49b1-b69e-a4e2da8c184e",
   "metadata": {},
   "source": [
    "**Your answer:**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afe69290-07c2-4f7f-8693-d05884b98963",
   "metadata": {},
   "source": [
    "#### **b(vi)**  \n",
    "Pick two lines from the dataset `friends_df`. Predict their character by applying the SGD classifier from **b(iv)** to their Set of Words encodings. Show the original lines, their original characters and the predicted characters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a7621f1-8fc9-4606-9c19-0fb007e13b7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eccc83fe-08a7-4109-a68d-765799874a83",
   "metadata": {},
   "source": [
    "### c) Doc2Vec (1.5 pts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cca937f-cc73-496b-90df-3d571b4ebd3b",
   "metadata": {},
   "source": [
    "#### **c(i)**  \n",
    "In this part, we want to encode the lines using Doc2Vec. Create a Doc2Vec embedding based on the documents in the preprocessed corpus `corpus_p`. Set the vector dimension to 10 and min_count to 3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bcd41ee-d39a-43a3-a7c0-eb2e6f1c9a29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fee6650f-1ca5-47aa-adfd-56c33c6dc4fd",
   "metadata": {},
   "source": [
    "#### **c(ii)**  \n",
    "Pick one (any) line from the dataset `friends_df`. Display the line and the character saying it. Find its most similar line w.r.t. the Doc2Vec encoding and display the original line and its corresponding character. Do the lines belong to the same character?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f543244-67a5-4d00-aff1-9a5a05518ad8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "437b009f-0d60-4f3c-857a-0ff2c62763af",
   "metadata": {},
   "source": [
    "**Your answer:**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aae3f695-65a1-4477-8909-517ec5133e5c",
   "metadata": {},
   "source": [
    "### d) Language model using N-grams (2 pts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba2d9dad-ffc3-4ae0-8c31-7ac550656871",
   "metadata": {},
   "source": [
    "#### For the following tasks, use the `friends_df` data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5da82a84-2f4b-44e6-9138-deab47dbeb92",
   "metadata": {},
   "source": [
    "#### **d(i)**  \n",
    "For each character, create a corresponding corpus. Each corpus must be a list of documents. Each document corresponds to one \"line\" value of that character and it should be a list of terms. You must obtain this list of terms after applying preprocessing steps such as to lowercase, no punctuation, and tokenization to the original line. Do not perform stemming/lemmatization and/or stopword removal for this task.\n",
    "Display the corpus of one of the characters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de295c77-0ef1-423e-ac2d-2dd4241d22c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e162de2-271d-4031-858b-40f5dcbc5506",
   "metadata": {},
   "source": [
    "#### **d(ii)**  \n",
    "For each character separately, build a trigram language model using MLE. Use both right and left padding and learn each language model using the character's corpus from **d(ii)**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "838f330b-20b3-4c1a-af32-6a84d45330fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e734c4b-2c31-4f19-9f43-4cf2983d8076",
   "metadata": {},
   "source": [
    "#### **d(iv)**  \n",
    "For each character, use the created trigram language model to generate a sentence of ten words. Display the sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "418add81-ad45-4712-905d-4ae71010f251",
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "788da653-9059-4045-9341-da119995bc4c",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Question 5: Process Mining (22pts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75571aa7-6567-44de-86e3-27aba8d5d03f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pm4py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "470d33e5-5fea-4f90-baba-dfc4c7fc72bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pm4py.statistics.traces.generic.log import case_statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4472e969-94fb-47e1-bb32-6dcfd7f154b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pm4py.algo.conformance.tokenreplay import algorithm as token_based_replay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8da87cd3-a2f0-46ee-97e2-c3c3c9905140",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1fd03f7-1c78-45be-9fb1-018779e47fd5",
   "metadata": {
    "tags": []
   },
   "source": [
    "### a) Loading the Data and Basic Statistics (9pts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e696437-3c4a-4e5f-9725-b12d153d1688",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### **a(i)** \n",
    "Load the data **reimburse.csv** and create a PM4Py event log. In doing so, use the following column mapping:\n",
    " - *Activity* is the activity key\n",
    " - *Case* is the case ID\n",
    " - *Timestamp* is the timestamp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d14d031-f77b-4b7c-807a-b352e2e4a7d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1cdcace-82db-451c-8d8a-2fe3d7033df2",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### **a(ii)** \n",
    "Compute and print the following basic information:\n",
    "- Number of events\n",
    "- Number of cases\n",
    "- Earliest timestamp\n",
    "- Latest timestamp\n",
    "- Number of trace variants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04d1b421-7308-4031-a5b2-bbf4cbfa18e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a45f421-ded8-4ebc-bfcd-bde5d3e3069b",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### **a(iii)** \n",
    "In process mining, multiple events that have the same timestamp can cause problems because the ordering of events (or even activities) becomes unclear. Moreover, they can indicate batching (i.e., one activity is executed for multiple cases simultaneously). Therefore, during your analysis, it is good to keep that in mind. To this end, compute the following statistics/answer the following questions:\n",
    "\n",
    "1. How many events occur almost at the same time (i.e., within less than 100ms as the preceding event. (Proceeding event in the *entire* event log)?\n",
    "2. Are there resources that complete two activities at the same time (within less than 100ms)?\n",
    "3. How many cases are there in which two activities are executed at the same time (i.e., two events that belong to the same case occur within less than 100ms)?\n",
    "\n",
    "*Hint: Depending on how you find the answers, be careful about event orderings.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70064816-159a-4092-92ec-d7d59e21c89f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c75d70c8-9b9a-4444-bd50-3e6d78c25bc6",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### **a(iv)** \n",
    "Provide a plot that shows the number of running cases (i.e., cases that have started but not yet finished) over time. You may assume that the log only contains complete traces. For each case that has started, its completion is the last observed event associated with that case. In case multiple cases start or end at the same time, you also generate multiple values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "522aaab1-e228-430a-bafd-3d1e82101aaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e9e3d73-265e-4161-8bfe-48a16643d42c",
   "metadata": {},
   "source": [
    "**Your answer**: *(Briefly describe the differences between the two models in about five sentences here.)*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e71e62b5-cc4e-4f3c-b836-08455477b93f",
   "metadata": {
    "tags": []
   },
   "source": [
    "### b) Discovery and Conformance Checking (9pts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fe3d450-49b1-479f-9e99-9760f8b9bd14",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### **b(i)** \n",
    "Mine a Petri net using Inductive Miner and display it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6417094f-6d92-4f1c-b63c-57588f16be5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d32b33dd-e8f5-46a7-be13-4821fbbf9e56",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### **b(ii)** \n",
    "Compute the fitness of the discovered Petri net using token-based replay.\n",
    "\n",
    "*Hint: PM4Py can directly (using the top-level API) compute the number of missing, remaining, consumed, and produced tokens. Based on these, you can, for example, compute the token-based replay fitness.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1aa42fb9-c4f2-4bec-a89a-308bef0b026c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16ab43b6-d08b-4b09-8b96-54b911b9c34f",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### **b(iii)** \n",
    "Filter the log to contain only traces where *Register Low* occurs. How many traces does the resulting log `log_low` contain?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faba7573-fcc6-46a7-ae70-35111ba8e5ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc3ef414-44a5-4a73-906f-36f673cfb6d5",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### **b(iv)** \n",
    "Discover a Petri net for `log_low` and compute its fitness. How does this model differ from the model you discovered in *b(i)*? \n",
    "\n",
    "Suppose each of your produced process models is considered a 2-class classifier: provided a trace, it returns \"Yes\" if and only if the trace can be replayed by the model. Based on this perspective, how would the two process models compare in terms of precision?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cc94f06-ad65-408d-ad9e-e8b114838b50",
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74965d2a-c969-438d-b1f7-a8123142b789",
   "metadata": {},
   "source": [
    "**Your answer:** *(Briefly describe the difference between the two models here. About two sentences can be enough.*)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9833080-54f8-4e1b-baea-810962a98281",
   "metadata": {},
   "source": [
    "**Your answer**: *(Relate your observations to precision here.)*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1c73b63-a4a4-4438-a8e4-d2ebfe276051",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Conformance Diagnostics De-jure Model\n",
    "The process owner provides you a de-jure model (i.e., a model of the should-be process) and a slightly changed version of the so far considered event log. \n",
    "\n",
    "In this task, you will again apply conformance checking by means of token-based replay to provide diagnostics on deviations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "390eabfa-faab-4bea-88ae-7c39bcad1ba0",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### **b(v)** \n",
    "Load the Petri net *pn_conf.apnml*, the event log *log_conf.xes*, and provide the overall (i.e., model-based) token-based replay fitness score.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98673f6e-e70e-4bc5-969b-057e954c58e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4b101e2-9b42-481f-a5f2-3c066416a131",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### **b(vi)** \n",
    "To provide additional diagnostics on the deviations, compute the missing, consumed, produced and remaining number of tokens for **each place**. \n",
    "To do so, use the following *pm4py* code:\n",
    "    \n",
    "    from pm4py.algo.conformance.tokenreplay import algorithm as token_based_replay\n",
    "\ttbr_results, place_fitness, transition_fitness, notexisting_activities_in_model =\n",
    "    token_based_replay.apply(log_conf, net_conf, im_conf, fm_conf, parameters={\"enable_pltr_fitness\": True, \"disable_variants\": True})\n",
    "\n",
    "After running this line for log `log_conf`, Petri net `net_conf` with initial marking `im_conf` and final marking `fm_conf`, the variable `place_fitness` will contain the token counts for each place and trace. Therefore, you will only need aggregate over the traces.\n",
    "Print a table of the token counts per place. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9a01c3b-4be6-4ea3-a0c7-3572d5bd696f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e098ac37-4978-4520-9d39-51462a1f5038",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### **b(vii)** \n",
    "Consider the token counts per place and a few unfitting traces, which deviation(s) do you observe? Describe the deviation and briefly explain how it can be related to the token counts of the individual places. For example, activity *xy* is often missing resulting in a high number of missing tokens in place *p*.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02586824-a2d3-4d36-b181-6e70fc3e2b54",
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cec26df-0110-41a8-a427-eeeea611fd7c",
   "metadata": {},
   "source": [
    "**Your answer:** *(Describe the deviation(s). One sentence can already be enough.)*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49fdc361-bfa7-49d1-809f-4509b427d8da",
   "metadata": {},
   "source": [
    "**Your answer:** (*Relate the deviation(s) to the token counts of the individual places. Roughly five sentences can be enough for a precise description.*)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58dcdf9d-c227-4436-b9c3-d45c5b6f7de4",
   "metadata": {
    "tags": []
   },
   "source": [
    "### c) Analyzing Fraud (4pts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2048349d-ebf9-48cc-af0b-9da5a2828b23",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### **c(i)** \n",
    "Filter the event log so that it only contains traces where a fraud report is filled (occurrence of `Fill Fraud Report`). For theses traces, create a bar plot showing the number of products of a certain brand involved in the fraud. Describe the resulting plot.\n",
    "\n",
    "*Hint: Each case is associated with precisely one brand.*\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b55a45be-a26c-4098-b23a-29b85102a715",
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca63f184-9cb3-48a0-abd4-b4146f1ebcaa",
   "metadata": {},
   "source": [
    "**Your answer:** (*Describe the plot in two to three sentences.*)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c635a50b-a7d9-433c-96c8-aae526f9e459",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### **c(ii)**  \n",
    "The plot shows differences between brands. Discuss the result. Consider what you learned in Lecture 11 (association rules). Try to provide additional analysis results to underpin your discussion.\n",
    "\n",
    "*Hint: A very short additional analysis (i.e., a few lines of code) might already be sufficient.*\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a32ba47d-c41f-418a-8b6b-b8176171b0a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code (for a short additional analysis)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81d49c56-6d9e-4b11-9fa4-d623f87a2f9a",
   "metadata": {},
   "source": [
    "**Your answer:** *(Relate your results to Lecture 11, approximately one short paragraph)*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63da1709-2452-4ac2-a162-ea5702d1694a",
   "metadata": {},
   "source": [
    "## Question 6 - Simpson's Paradox (3 pts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dd0ddaa-03b6-4195-923b-6ce0e73b02f0",
   "metadata": {},
   "source": [
    "### Sex Bias in Berkeley Graduate Admissions?\n",
    "\n",
    "In the Fall of 1973, the University of California at Berkeley released data about their graduate class. The data showed the major the applicant applied to, their self-reported gender (Male or Female), and whether or not they were accepted or rejected. The acceptance rates between men and women were different. This caused immediate concern in the public as people thought that Berkeley was biased against women."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04df6ab4-e21f-4265-8d40-077bff5d5baa",
   "metadata": {},
   "source": [
    "The \"Berkeley Dataset\" contains all 12,763 applicants to UC-Berkeley's graduate programs in Fall 1973. This dataset was published by UC-Berkeley researchers in an analysis to understand the possible gender bias in admissions.\n",
    "\n",
    "Dataset Variables:\n",
    "\n",
    "Year : number ➜ The application year (this value is always 1973)\n",
    "\n",
    "Major : string ➜: An anonymized major code (either A, B, C, D, E, F, or Other). The specific majors are unknown except that A-F are the six majors with the most applicants in Fall 1973\n",
    "\n",
    "Gender : string ➜ Applicant self-reported gender (either M or F)\n",
    "\n",
    "Admission: string ➜ Admission decision (either Rejected or Accepted)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "058897ab-2f92-4f71-b00c-ec3109d63687",
   "metadata": {},
   "source": [
    "**a)**\n",
    "Upload the data from the `berkeley.csv` file and load it into a dataframe named `data`. Display the first few lines from the dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a34afacf-87c8-472b-86ae-728311145b18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b5bd1eb-0c70-4e11-b0b9-fa132dbebdd3",
   "metadata": {},
   "source": [
    "**b)** Remove the \"Year\" column as it does not contain any information in this dataset (all years are 1973.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db47b833-2b21-473e-a2ae-93edceb1478c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef777de3-cdcd-474d-ba69-d8c206bfd46e",
   "metadata": {},
   "source": [
    "**c)** For each of the values of column \"Gender\", compute the admission rate and compare them against each other."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f038e712-b3d7-4e28-a809-8506d0ff8847",
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dc2a3b7-659b-4710-91b1-230e19206a82",
   "metadata": {},
   "source": [
    "**Your answer:**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20439634-fed0-47fa-9ca6-68c9cf001f7c",
   "metadata": {},
   "source": [
    "**d)** For each value combination of the \"Gender\" and \"Major\" columns, compute the admission rate. Compare the admission rate of women against the admission rate of men for each of the majors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "480b4533-2741-4433-a74b-8ec7bb4bb494",
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9daa585b-c647-499b-9afb-78e1c3a2502e",
   "metadata": {},
   "source": [
    "**Your answer:**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78625577-641f-4f34-a633-bae183fee886",
   "metadata": {},
   "source": [
    "**e)** Can you confirm there is a sex bias in the admission rates of the students?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acf0d7eb-e486-4b0e-96e9-da24e9bc8b12",
   "metadata": {},
   "source": [
    "**Your answer:**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "927712ec-fd3c-4561-8732-c3babeb514f3",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Question 7: Big Data (15pts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e67686c-0788-4b91-bcf0-70b8e8738aab",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "193ba79e-e377-41e0-84b1-d61fc81f0bb1",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "You are a data scientist at NASA, an agency for civil space programs, and with your team you develop and maintain the software of the NASA Crew Exploration Vehicle (CEV).  Your task is to analyze the performance of the software, and as a first exploratory step, you would like to **compute the mean execution times** of function calls within that software. Since the running vehicle will generate a high throughput of observable events in a stream, you decide to set up a MapReduce pipeline in Hadoop. \n",
    "\n",
    "The file **nasa-cev-software-tests.tsv** records timestamped events of the vehicle's software tests. The log contains the columns *Case*, *Activity* and *Timestamp*, denoting the case ID, the activity key (method call) and timestamp of the event record in nanoseconds, respectively. Furthermore, the log contains the columns *Lifecycle Transition* and *Execution ID*. The lifecycle transition takes either of the values *start* and *complete*, stating whether the corresponding activity (method call) in that row is being started or completed at the specified timestamp. The execution ID relates each event to a concrete method call, i.e., for each execution ID, there are exactly two entries (namely a *start* and a *complete* event) in the log."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5686ba3-56fa-4e35-9423-5374e47e7acf",
   "metadata": {
    "tags": []
   },
   "source": [
    "### a) Plan the Maths (2pt):"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c24cce6",
   "metadata": {},
   "source": [
    "The mean $\\mu_n$ over numerical values $v_1,...,v_n$ is well-known to be computed as $\\mu_n = \\frac{1}{n}\\sum_{i=1}^{n} v_i$.\\\n",
    "One may also use the alternative recursive formalization $\\mu_{n+m} = \\frac{n\\cdot\\mu_{n} + m\\cdot\\mu_{m}}{n+m}$. \\\n",
    "What is the advantage of using the alternative formalization when you think of handling streaming data or distributed data? Briefly explain.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4816e8fb-3968-4f34-8307-a2544fd551dc",
   "metadata": {},
   "source": [
    "**Your answer:**\n",
    "1. In case of streaming data, the mean can be computed in real time by keeping the number of seen values.\n",
    "2. In case of distributed data, the mean of all data can be computed from the mean of each nodes if the number of values of each node is known."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76ab1e45-757c-4dc4-a962-882fb3d6d95f",
   "metadata": {
    "tags": []
   },
   "source": [
    "### b) Set up MapReduce (10pts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d10d054f-512e-49a6-b935-85c80880368b",
   "metadata": {
    "tags": []
   },
   "source": [
    "Realize the computation of the mean execution times of activities as a MapReduce job. \n",
    "You need to implement this a cascaded MapReduced job. This means that the output of the first job will serve as the input of the second job. In the first job, derive the execution times of each activity execution, i.e. the time difference between the *complete* and the *start* lifecycle transition of each activity execution. In the second step, aggregate this timing information to compute the mean."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6653ff4-8c18-44d4-9de5-98ad8715425a",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### **b(i)** \n",
    "Specify the *function signatures* of the map functions and the reduce functions that you are going to use.\\\n",
    "I.e., find concrete sets to substitute $K_1, V_1, ... $ in the general signatures for map and reduce functions \\\n",
    "\n",
    "$ map:  K_1 \\times V_1 \\rightarrow (K_2 \\times V_2)^* $\\\n",
    "$ reduce: K_2 \\times (V_2)^* \\rightarrow (V_3)^*$ (or a singleton $V_3$) \n",
    "\n",
    "*Hint: You may introduce symbols to denote sets, e.g. $Act$ for the set of activities.\\\n",
    "You may also first implement the solution (b(ii)) to get an idea about the underlying signatures.\\\n",
    "Mind that you need two map and two reduce functions.*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d24616b-2f62-4317-950c-20956c8a50bc",
   "metadata": {
    "tags": []
   },
   "source": [
    "**Your answer:**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7511e48-b42e-4236-8232-aef6fccec140",
   "metadata": {},
   "source": [
    "#### **b(ii)**: \n",
    "Specify map functions and reduce functions to compute the mean execution time per activity as python scripts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b833afd8-718b-41aa-b54b-4193404477fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code (nasa_mapper1.py)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eac87097-d200-4871-a3a3-513ea6a635a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code (nasa_reducer1.py)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "191be80c-2ba2-45af-90fa-cff451bb656d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code (nasa_mapper2.py)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f15c173-dd64-4d9f-b477-c20e16ae265f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code (nasa_reducer2.py)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b2739a1-c55f-4e7e-b0d2-12bcee4f3333",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9109b6f2-cc6f-41b1-8274-f651f7b663bc",
   "metadata": {
    "tags": []
   },
   "source": [
    "### c) Run MapReduce (3pts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0de07da6-979f-4f2a-a331-843259636a0c",
   "metadata": {},
   "source": [
    "In the following, please use one of your team members' matriculation number as an identifier."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94b8fbac-5766-420a-9595-423f0b092d73",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### **c(i) (Randomization)**: \n",
    "Before applying your functions from the previous step to the dataset, please insert the matriculation number and run the following lines to randomly filter out a few of the traces in the event log, and continue working with the filtered log. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e139cb0-b347-46be-a070-ae9447fe010a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# your matriculation number here\n",
    "matr_nb = # ... #\n",
    "\n",
    "# utility code (do not change)\n",
    "import random\n",
    "random.seed(matr_nb)\n",
    "\n",
    "full_df = pd.read_csv(\"datasets/nasa-cev-software-tests.tsv\", sep=\"\\t\")\n",
    "\n",
    "case_ids = list(set(full_df[\"Case\"].values))\n",
    "case_ids.sort()\n",
    "filtered_out_case_ids = random.sample(case_ids, 10)\n",
    "filtered_case_ids = [case_id for case_id in case_ids if case_id not in filtered_out_case_ids]\n",
    "randomized_df = full_df[full_df[\"Case\"].isin(filtered_case_ids)]\n",
    "\n",
    "randomized_df.to_csv(\"datasets/nasa-cev-software-tests-randomized-\" + str(matr_nb) + \".tsv\",\n",
    "          columns=[\"Activity\", \"Timestamp\", \"Lifecycle Transition\", \"Execution ID\"],\n",
    "          sep=\"\\t\", index=False, header=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7aa8989a-1d2e-4360-888a-732a269525dd",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### **c(ii)**:\n",
    "Now, it is time to work with the Hadoop Distributed File System (HDFS). Follow the instructions below and show your results in each step (screenshots of the command line).\n",
    "\n",
    "    1) Import the event log to your Docker engine (at /usr/local/hadoop/(your_matr_nb)-event-log/). You also need to import the python scripts, but only document the event log import here.\n",
    "    2) Upload the files to the running HDFS (at /input/(your_matr_nb)-event-log/).\n",
    "\t3) Run Hadoop commands for the MapReduce computation.\n",
    "    4) Show the final output (computed mean execution times)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d3b0d41-bf64-43a8-a297-94da3a389f37",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image\n",
    "# your code\n",
    "# Image(filename='filename_screenshot_of_a1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ca0a837-d297-4bb9-b377-d9744764fc52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code\n",
    "# Image(filename='filename_screenshot_of_a2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fcbd059-5813-4bf8-95ca-a5a06c3e32c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code\n",
    "# Image(filename='filename_screenshot_of_a3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7141831-06ce-41a8-a6e2-c31de6e775af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code\n",
    "# Image(filename='filename_screenshot_of_a4')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "vscode": {
   "interpreter": {
    "hash": "ce62beefdc5911faf9e29e9dc2cfefdca3b26162f1fe8f3f0595eab6ba6f1e86"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
